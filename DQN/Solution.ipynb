{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import graphical, game\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of errors happens in action conversion:  0\n"
     ]
    }
   ],
   "source": [
    "N_Rows = 10\n",
    "N_Cols = 8\n",
    "N_Dir = 2\n",
    "\n",
    "report_frequency = 100\n",
    "\n",
    "All_Actions = np.arange(0,N_Rows*N_Cols*N_Dir)\n",
    "\n",
    "N_All_Actions = len(All_Actions)\n",
    "N_State = N_Rows * N_Cols + 1 # board game state + moves_left\n",
    "\n",
    "def get_state(board, moves_left):\n",
    "    state = np.zeros(len(board) - (N_Rows-1) + 1)\n",
    "    \n",
    "    c_state_index = 0\n",
    "    for s in range(0,len(board)):\n",
    "        if board[s] != '\\n':\n",
    "            #print(board[s])\n",
    "            state[c_state_index] = ord(board[s]) - ord('a')\n",
    "            state[c_state_index] = -1 if state[c_state_index] < 0 else state[c_state_index]\n",
    "            c_state_index += 1\n",
    "            \n",
    "    state[-1] = moves_left\n",
    "    return state\n",
    "\n",
    "def get_action_from(move):\n",
    "    action = np.array(move)\n",
    "    \n",
    "    if move[2]:\n",
    "        action[2] = 1\n",
    "    else:\n",
    "        action[2] = 0\n",
    "    \n",
    "    out_action = (action[2]) * (N_Rows * N_Cols) + (action[0] * N_Rows + action[1])\n",
    "    \n",
    "    return out_action\n",
    "\n",
    "def get_move_from(action):\n",
    "    row_col = action % (N_Rows * N_Cols)\n",
    "    \n",
    "    dir = int(action / (N_Rows * N_Cols))\n",
    "    \n",
    "    return (int(row_col / N_Rows), row_col % N_Rows, dir >= 1)\n",
    "\n",
    "# test action conversion\n",
    "num_error_in_conversion = 0\n",
    "for i in range(0,160):\n",
    "    a = get_move_from(i)\n",
    "    ii = get_action_from(a)\n",
    "    if i != ii:\n",
    "        num_error_in_conversion += 1\n",
    "print(\"number of errors happens in action conversion: \", num_error_in_conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork():\n",
    "    \"\"\"Q-Value Estimator neural network.\n",
    "\n",
    "    This network is used for both the Q-Network and the Target Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scope=\"estimator\", summaries_dir=None):\n",
    "        self.scope = scope\n",
    "        # Writes Tensorboard summaries to disk\n",
    "        self.summary_writer = None\n",
    "        with tf.variable_scope(scope):\n",
    "            # Build the graph\n",
    "            self._build_model()\n",
    "            if summaries_dir:\n",
    "                summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n",
    "                if not os.path.exists(summary_dir):\n",
    "                    os.makedirs(summary_dir)\n",
    "                self.summary_writer = tf.summary.FileWriter(summary_dir)\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Builds the Tensorflow graph.\n",
    "        \"\"\"\n",
    "\n",
    "        # Placeholders for our input\n",
    "        # Our inputs are board game state with shape of (None, N_State)\n",
    "        self.X_pl = tf.placeholder(shape=[None, N_State], dtype=tf.uint8, name=\"X\")\n",
    "        # The TD target value\n",
    "        self.y_pl = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
    "        # Integer id of which action was selected\n",
    "        self.actions_pl = tf.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n",
    "\n",
    "        X = (tf.to_float(self.X_pl) + 1) / 5.0 # normalize input between (0,1)\n",
    "        batch_size = tf.shape(self.X_pl)[0]\n",
    "\n",
    "        # Three fully connected layers\n",
    "        fully1 = tf.contrib.layers.fully_connected(X, 100, activation_fn=tf.nn.relu)      # 80 to 100\n",
    "        fully2 = tf.contrib.layers.fully_connected(fully1, 120, activation_fn=tf.nn.relu) # 100 to 120\n",
    "        fully3 = tf.contrib.layers.fully_connected(fully2, 140, activation_fn=tf.nn.relu) # 120 to 140\n",
    "\n",
    "        # output layers\n",
    "        self.predictions = tf.contrib.layers.fully_connected(fully3, N_All_Actions)  # 140 to 160\n",
    "\n",
    "        # Get the predictions for the chosen actions only\n",
    "        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.actions_pl\n",
    "        self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n",
    "\n",
    "        # Calculate the loss\n",
    "        self.losses = tf.squared_difference(self.y_pl, self.action_predictions)\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "\n",
    "        # Optimizer Parameters from original paper\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
    "        self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "\n",
    "        # Summaries for Tensorboard\n",
    "        self.summaries = tf.summary.merge([\n",
    "            tf.summary.scalar(\"loss\", self.loss),\n",
    "            tf.summary.histogram(\"loss_hist\", self.losses),\n",
    "            tf.summary.histogram(\"q_values_hist\", self.predictions),\n",
    "            tf.summary.scalar(\"max_q_value\", tf.reduce_max(self.predictions))\n",
    "        ])\n",
    "\n",
    "    def predict(self, sess, s):\n",
    "        \"\"\"\n",
    "        Predicts action values.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session\n",
    "          s: State input of shape [batch_size, N_State]\n",
    "\n",
    "        Returns:\n",
    "          Tensor of shape [batch_size, N_All_Actions] containing the estimated \n",
    "          action values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.predictions, { self.X_pl: s })\n",
    "\n",
    "    def update(self, sess, s, a, y, episode_num):\n",
    "        \"\"\"\n",
    "        Updates the estimator towards the given targets.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session object\n",
    "          s: State input of shape [batch_size, N_State]\n",
    "          a: Chosen actions of shape [batch_size]\n",
    "          y: Targets of shape [batch_size]\n",
    "\n",
    "        Returns:\n",
    "          The calculated loss on the batch.\n",
    "        \"\"\"\n",
    "        feed_dict = { self.X_pl: s, self.y_pl: y, self.actions_pl: a }\n",
    "        summaries, global_step, _, loss = sess.run(\n",
    "            [self.summaries, tf.contrib.framework.get_global_step(), self.train_op, self.loss],\n",
    "            feed_dict)\n",
    "        #if self.summary_writer and episode_num % report_frequency == 0:\n",
    "        #    self.summary_writer.add_summary(summaries, global_step)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetworkCopier():\n",
    "    \"\"\"\n",
    "    Copy model parameters of one estimator to another.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, estimator1, estimator2):\n",
    "        \"\"\"\n",
    "        Defines copy-work operation graph.  \n",
    "        Args:\n",
    "          estimator1: Estimator to copy the paramters from\n",
    "          estimator2: Estimator to copy the parameters to\n",
    "        \"\"\"\n",
    "        e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)]\n",
    "        e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "        e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)]\n",
    "        e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "\n",
    "        self.update_ops = []\n",
    "        for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "            op = e2_v.assign(e1_v)\n",
    "            self.update_ops.append(op)\n",
    "            \n",
    "    def make(self, sess):\n",
    "        \"\"\"\n",
    "        Makes copy.\n",
    "        Args:\n",
    "            sess: Tensorflow session instance\n",
    "        \"\"\"\n",
    "        sess.run(self.update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "\n",
    "    Args:\n",
    "        estimator: An estimator that returns q values for a given state\n",
    "        nA: Number of actions in the environment.\n",
    "\n",
    "    Returns:\n",
    "        A function that takes the (sess, observation, epsilon) as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "\n",
    "    \"\"\"\n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "class deep_q_learning():\n",
    "    def __init__(self, sess,\n",
    "                 q_estimator,\n",
    "                 target_estimator,\n",
    "                 num_episodes,\n",
    "                 experiment_dir,\n",
    "                 replay_memory_size=500000,\n",
    "                 replay_memory_init_size=50000,\n",
    "                 update_target_estimator_every=10000,\n",
    "                 discount_factor=0.99,\n",
    "                 epsilon_start=1.0,\n",
    "                 epsilon_end=0.1,\n",
    "                 epsilon_decay_steps=500000,\n",
    "                 batch_size=32):\n",
    "        \"\"\"\n",
    "        Q-Learning algorithm for off-policy TD control using Function Approximation.\n",
    "        Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "\n",
    "        Args:\n",
    "            sess: Tensorflow Session object\n",
    "            q_estimator: Estimator object used for the q values\n",
    "            target_estimator: Estimator object used for the targets\n",
    "            num_episodes: Number of episodes to run for\n",
    "            experiment_dir: Directory to save Tensorflow summaries in\n",
    "            replay_memory_size: Size of the replay memory\n",
    "            replay_memory_init_size: Number of random experiences to sampel when initializing \n",
    "                                     the reply memory.\n",
    "            update_target_estimator_every: Copy parameters from the Q estimator to the \n",
    "                                           target estimator every N steps\n",
    "            discount_factor: Gamma discount factor\n",
    "            epsilon_start: Chance to sample a random action when taking an action.\n",
    "                           Epsilon is decayed over time and this is the start value\n",
    "            epsilon_end: The final minimum value of epsilon after decaying is done\n",
    "            epsilon_decay_steps: Number of steps to decay epsilon over\n",
    "            batch_size: Size of batches to sample from the replay memory\n",
    "        \"\"\"\n",
    "        \n",
    "        self.sess = sess\n",
    "        self.q_estimator = q_estimator\n",
    "        self.target_estimator = target_estimator\n",
    "        self.num_episodes = num_episodes\n",
    "        self.experiment_dir = experiment_dir\n",
    "        self.replay_memory_size = replay_memory_size\n",
    "        self.replay_memory_init_size = replay_memory_init_size\n",
    "        self.update_target_estimator_every = update_target_estimator_every\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay_steps = epsilon_decay_steps\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.cur_episode = 0\n",
    "        self.cur_t = 0\n",
    "        self.loss = None\n",
    "        \n",
    "        # The replay memory\n",
    "        self.replay_memory = []\n",
    "        \n",
    "        # Make model copier object\n",
    "        self.estimator_copy = QNetworkCopier(self.q_estimator, self.target_estimator)\n",
    "\n",
    "        # Keeps track of useful statistics\n",
    "        self.stats = {'q_net_loss':0, 'episode_rewards':0, 'epsilon':0, 'counter_observation':0, 'counter_episode':0}\n",
    "\n",
    "        # Create directories for checkpoints and summaries\n",
    "        self.checkpoint_dir = os.path.join(self.experiment_dir, \"checkpoints\")\n",
    "        self.checkpoint_path = os.path.join(self.checkpoint_dir, \"model\")\n",
    "\n",
    "        if not os.path.exists(self.checkpoint_dir):\n",
    "            os.makedirs(self.checkpoint_dir)\n",
    "\n",
    "        self.saver = tf.train.Saver()\n",
    "        # Load a previous checkpoint if we find one\n",
    "        latest_checkpoint = tf.train.latest_checkpoint(self.checkpoint_dir)\n",
    "        if latest_checkpoint:\n",
    "            print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "            self.saver.restore(self.sess, latest_checkpoint)\n",
    "\n",
    "        # Get the current time step\n",
    "        self.total_t = self.sess.run(tf.contrib.framework.get_global_step())\n",
    "\n",
    "        # The epsilon decay schedule\n",
    "        self.epsilons = np.linspace(self.epsilon_start, self.epsilon_end, self.epsilon_decay_steps)\n",
    "\n",
    "        # The policy we're following\n",
    "        self.policy = make_epsilon_greedy_policy(self.q_estimator, N_All_Actions)\n",
    "    \n",
    "    def collect_observation(self, board, move, score_delta, next_board, moves_left):\n",
    "        state = get_state(board, moves_left + 1)\n",
    "        reward = score_delta / 100.0\n",
    "        action = get_action_from(move)\n",
    "        n_state = get_state(next_board, moves_left)\n",
    "        done = (moves_left == 0)\n",
    "        \n",
    "        # If our replay memory is full, pop the first element\n",
    "        if len(self.replay_memory) == self.replay_memory_size:\n",
    "            self.replay_memory.pop(0)\n",
    "        \n",
    "        self.replay_memory.append(Transition(state, action, reward, n_state, done))\n",
    "        \n",
    "        if len(self.replay_memory) < self.replay_memory_init_size:\n",
    "            return\n",
    "        \n",
    "        # Update statistics\n",
    "        if self.loss is not None:\n",
    "            self.stats['counter_observation'] += 1\n",
    "            self.stats['epsilon'] += self.epsilons[min(self.total_t, self.epsilon_decay_steps-1)]\n",
    "            self.stats['episode_rewards'] += reward\n",
    "            self.stats['q_net_loss'] += self.loss\n",
    "        \n",
    "        self.cur_t += 1\n",
    "        return\n",
    "    \n",
    "    def predict_action(self, board, score, moves_left):\n",
    "        state = get_state(board, moves_left)\n",
    "        action_probs = self.policy(self.sess, state, self.epsilons[min(self.total_t, self.epsilon_decay_steps-1)])\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        \n",
    "        return get_move_from(All_Actions[action])\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.replay_memory) < self.replay_memory_init_size:\n",
    "            return\n",
    "        \n",
    "        # Sample a minibatch from the replay memory\n",
    "        samples = random.sample(self.replay_memory, self.batch_size)\n",
    "        states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n",
    "\n",
    "        # Calculate q values and targets\n",
    "        q_values_next = target_estimator.predict(self.sess, next_states_batch)\n",
    "        targets_batch = reward_batch \\\n",
    "                      + np.invert(done_batch).astype(np.float32) \\\n",
    "                      * self.discount_factor \\\n",
    "                      * np.amax(q_values_next, axis=1)\n",
    "\n",
    "        # Perform gradient descent update\n",
    "        states_batch = np.array(states_batch)\n",
    "        self.loss = q_estimator.update(sess, states_batch, action_batch, targets_batch, self.cur_episode)\n",
    "\n",
    "        self.total_t += 1\n",
    "        \n",
    "        # Maybe update the target estimator\n",
    "        if self.total_t % self.update_target_estimator_every == 0:\n",
    "            self.estimator_copy.make(self.sess)\n",
    "            print(\"\\nCopied model parameters to target network.\")\n",
    "\n",
    "        # Print out which step we're on, useful for debugging.\n",
    "        print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n",
    "                self.cur_t, self.total_t, self.cur_episode + 1, self.num_episodes, self.loss), end=\"\")\n",
    "    \n",
    "    def init_episode(self):\n",
    "        self.cur_t = 0\n",
    "        \n",
    "        if len(self.replay_memory) < self.replay_memory_init_size:\n",
    "            return\n",
    "        # Save the current checkpoint\n",
    "        self.saver.save(self.sess, self.checkpoint_path)\n",
    "        \n",
    "        # Add summaries to tensorboard\n",
    "        if self.stats['counter_observation'] > 1000:\n",
    "            episode_summary = tf.Summary()\n",
    "            episode_summary.value.add(\n",
    "                simple_value=self.stats['epsilon'] / self.stats['counter_observation'], tag=\"episode/epsilon\")\n",
    "            episode_summary.value.add(\n",
    "                simple_value=self.stats['episode_rewards'] / self.stats['counter_episode'], tag=\"episode/reward\")\n",
    "            episode_summary.value.add(\n",
    "                simple_value=self.stats['q_net_loss'] / self.stats['counter_observation'], tag=\"QNetLoss\")\n",
    "            q_estimator.summary_writer.add_summary(episode_summary, self.cur_episode)\n",
    "            q_estimator.summary_writer.flush()\n",
    "            \n",
    "            self.stats['counter_observation'] = 0\n",
    "            self.stats['counter_episode'] = 0\n",
    "            self.stats['epsilon'] = 0\n",
    "            self.stats['episode_rewards'] = 0\n",
    "            self.stats['q_net_loss'] = 0\n",
    "        \n",
    "        # Reset\n",
    "        self.cur_episode += 1\n",
    "        self.stats['counter_episode'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-f7fee15a62aa>:54: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_global_step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kooro\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 81170877550159050235589795067197456223\n",
      "Step 23 (9999) @ Episode 401/10000, loss: 74.680908203125254\n",
      "Copied model parameters to target network.\n",
      "Step 23 (19999) @ Episode 801/10000, loss: 22.568321228027344\n",
      "Copied model parameters to target network.\n",
      "Step 23 (29999) @ Episode 1201/10000, loss: 17.420785903930664\n",
      "Copied model parameters to target network.\n",
      "Step 23 (39999) @ Episode 1601/10000, loss: 17.616834640502932\n",
      "Copied model parameters to target network.\n",
      "Step 23 (49999) @ Episode 2001/10000, loss: 28.768297195434572\n",
      "Copied model parameters to target network.\n",
      "Step 23 (59999) @ Episode 2401/10000, loss: 10.950723648071289\n",
      "Copied model parameters to target network.\n",
      "Step 23 (69999) @ Episode 2801/10000, loss: 11.847319602966309\n",
      "Copied model parameters to target network.\n",
      "Step 23 (79999) @ Episode 3201/10000, loss: 9.4709930419921887\n",
      "Copied model parameters to target network.\n",
      "Step 23 (89999) @ Episode 3601/10000, loss: 12.173801422119148\n",
      "Copied model parameters to target network.\n",
      "Step 23 (99999) @ Episode 4001/10000, loss: 5.6415576934814454\n",
      "Copied model parameters to target network.\n",
      "Step 23 (109999) @ Episode 4401/10000, loss: 13.958000183105469\n",
      "Copied model parameters to target network.\n",
      "Step 23 (119999) @ Episode 4801/10000, loss: 11.138745307922363\n",
      "Copied model parameters to target network.\n",
      "Step 23 (129999) @ Episode 5201/10000, loss: 7.7094488143920954\n",
      "Copied model parameters to target network.\n",
      "Step 23 (139999) @ Episode 5601/10000, loss: 10.260759353637695\n",
      "Copied model parameters to target network.\n",
      "Step 23 (149999) @ Episode 6001/10000, loss: 30.449352264404297\n",
      "Copied model parameters to target network.\n",
      "Step 23 (159999) @ Episode 6401/10000, loss: 9.4962825775146485\n",
      "Copied model parameters to target network.\n",
      "Step 23 (169999) @ Episode 6801/10000, loss: 16.403987884521484\n",
      "Copied model parameters to target network.\n",
      "Step 23 (179999) @ Episode 7201/10000, loss: 6.8461952209472665\n",
      "Copied model parameters to target network.\n",
      "Step 23 (189999) @ Episode 7601/10000, loss: 11.804327011108398\n",
      "Copied model parameters to target network.\n",
      "Step 23 (199999) @ Episode 8001/10000, loss: 12.523483276367188\n",
      "Copied model parameters to target network.\n",
      "Step 23 (209999) @ Episode 8401/10000, loss: 9.8507747650146485\n",
      "Copied model parameters to target network.\n",
      "Step 23 (219999) @ Episode 8801/10000, loss: 18.320865631103516\n",
      "Copied model parameters to target network.\n",
      "Step 23 (229999) @ Episode 9201/10000, loss: 11.038505554199219\n",
      "Copied model parameters to target network.\n",
      "Step 23 (239999) @ Episode 9601/10000, loss: 18.828851699829106\n",
      "Copied model parameters to target network.\n",
      "Step 23 (249999) @ Episode 10001/10000, loss: 16.394681930541992\n",
      "Copied model parameters to target network.\n",
      "Step 23 (259999) @ Episode 10401/10000, loss: 16.502573013305664\n",
      "Copied model parameters to target network.\n",
      "Step 23 (269999) @ Episode 10801/10000, loss: 25.629325866699227\n",
      "Copied model parameters to target network.\n",
      "Step 23 (279999) @ Episode 11201/10000, loss: 147.48001098632812\n",
      "Copied model parameters to target network.\n",
      "Step 23 (289999) @ Episode 11601/10000, loss: 27.440116882324222\n",
      "Copied model parameters to target network.\n",
      "Step 23 (299999) @ Episode 12001/10000, loss: 31.111801147460938\n",
      "Copied model parameters to target network.\n",
      "Step 23 (309999) @ Episode 12401/10000, loss: 138.88394165039062\n",
      "Copied model parameters to target network.\n",
      "Step 23 (319999) @ Episode 12801/10000, loss: 36.496669769287117\n",
      "Copied model parameters to target network.\n",
      "Step 23 (329999) @ Episode 13201/10000, loss: 37.543205261230474\n",
      "Copied model parameters to target network.\n",
      "Step 4 (338880) @ Episode 13557/10000, loss: 72.0765762329101665"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b152c6cd7983>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[0mspeedup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraphical\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mai_callback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransition_callback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_of_game_callback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspeedup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m     \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\OneDrive\\Desktop\\Ubisoft\\RedLynx_ML_Assignment\\graphical.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    276\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mST_POSTGAME\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_postgame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m             \u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "global my_dqn\n",
    "\n",
    "def ai_callback(board, score, moves_left):\n",
    "    global my_dqn\n",
    "    \n",
    "    predicted_move = my_dqn.predict_action(board, score, moves_left)\n",
    "    #print(predicted_move)\n",
    "    \n",
    "    #dir = random.randint(0, 1) == 0\n",
    "    #return (random.randint(0, 7 if dir else 6), random.randint(0, 8 if dir else 9), dir)\n",
    "    return predicted_move\n",
    "\n",
    "def transition_callback(board, move, score_delta, next_board, moves_left):\n",
    "    global my_dqn\n",
    "    \n",
    "    my_dqn.collect_observation(board, move, score_delta, next_board, moves_left)\n",
    "    my_dqn.update()\n",
    "    \n",
    "    pass # This can be used to monitor outcomes of moves\n",
    "\n",
    "def end_of_game_callback(boards, scores, moves, final_score):\n",
    "    global my_dqn\n",
    "    \n",
    "    my_dqn.init_episode()\n",
    "    \n",
    "    return True # True = play another, False = Done\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    global my_dqn\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Where we save our checkpoints and graphs\n",
    "    experiment_dir = os.path.abspath(\"./experiments/{}\".format(\"ubisoft-game\"))\n",
    "\n",
    "    # Create a glboal step variable\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "    # Create estimators\n",
    "    q_estimator = QNetwork(scope=\"q_estimator\", summaries_dir=experiment_dir)\n",
    "    target_estimator = QNetwork(scope=\"target_q\")\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    my_dqn = deep_q_learning(sess,\n",
    "                             q_estimator=q_estimator,\n",
    "                             target_estimator=target_estimator,\n",
    "                             experiment_dir=experiment_dir,\n",
    "                             num_episodes=10000,\n",
    "                             replay_memory_size=500000,\n",
    "                             replay_memory_init_size=50000,\n",
    "                             update_target_estimator_every=10000,\n",
    "                             epsilon_start=1.0,\n",
    "                             epsilon_end=0.1,\n",
    "                             epsilon_decay_steps=500000,\n",
    "                             discount_factor=0.99,\n",
    "                             batch_size=32)\n",
    "\n",
    "    speedup = 1000.0\n",
    "    g = graphical.Game(ai_callback, transition_callback, end_of_game_callback, speedup)\n",
    "    g.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
