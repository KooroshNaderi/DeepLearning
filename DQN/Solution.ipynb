{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "#import tensorflow as tf\n",
    "import random\n",
    "import graphical, game\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159]\n",
      "[70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 89, 99, 109, 119, 129, 139, 149, 159]\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  80  81\n",
      "  82  83  84  85  86  87  88  90  91  92  93  94  95  96  97  98 100 101\n",
      " 102 103 104 105 106 107 108 110 111 112 113 114 115 116 117 118 120 121\n",
      " 122 123 124 125 126 127 128 130 131 132 133 134 135 136 137 138 140 141\n",
      " 142 143 144 145 146 147 148 150 151 152 153 154 155 156 157 158]\n",
      "[[ 3.  0. -1.  1.  1. -1.  0.  2.]\n",
      " [-1.  1.  1.  2.  2.  1.  1.  3.]\n",
      " [-1.  2.  3. -1.  0. -1.  3. -1.]\n",
      " [-1.  2. -1.  3. -1.  3.  3.  2.]\n",
      " [-1. -1.  1.  0. -1. -1.  1.  2.]\n",
      " [ 0.  2.  0.  3.  2. -1.  2. -1.]\n",
      " [-1.  3. -1. -1.  2.  2. -1. -1.]\n",
      " [ 2. -1.  2.  1.  3.  0.  2.  3.]\n",
      " [ 3.  2.  0. -1.  3. -1.  1. -1.]\n",
      " [ 3.  3. -1.  3.  2.  2.  3.  1.]]\n",
      "[[0. 0. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "number of errors happens in action conversion:  0\n"
     ]
    }
   ],
   "source": [
    "N_Rows = 10\n",
    "N_Cols = 8\n",
    "N_Dir = 2\n",
    "\n",
    "def match_in_row_from_index(board_num, i, j, flag_print):\n",
    "    count_equal_forward = 1\n",
    "    count_equal_backward = 0\n",
    "    c1 = board_num[i, j]\n",
    "    if c1 < 0:\n",
    "        return False\n",
    "    \n",
    "    if j + 1 < N_Cols and c1 == board_num[i, j + 1]:\n",
    "        count_equal_forward += 1\n",
    "        if j + 2 < N_Cols and c1 == board_num[i, j + 2]:\n",
    "            count_equal_forward += 1\n",
    "    \n",
    "    if j - 1 >= 0 and c1 == board_num[i, j - 1]:\n",
    "        count_equal_backward += 1\n",
    "        if j - 2 >= 0 and c1 == board_num[i, j - 2]:\n",
    "            count_equal_backward += 1\n",
    "    if flag_print:\n",
    "        print(count_equal_backward + count_equal_forward)\n",
    "    \n",
    "    if count_equal_backward + count_equal_forward >= 3:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def match_in_col_from_index(board_num, i, j, flag_print):\n",
    "    count_equal_forward = 1\n",
    "    count_equal_backward = 0\n",
    "    r1 = board_num[i, j]\n",
    "    if r1 < 0:\n",
    "        return False\n",
    "    \n",
    "    if i + 1 < N_Rows and r1 == board_num[i + 1, j]:\n",
    "        count_equal_forward += 1\n",
    "        if i + 2 < N_Rows and r1 == board_num[i + 2, j]:\n",
    "            count_equal_forward += 1\n",
    "    \n",
    "    if i - 1 >= 0 and r1 == board_num[i - 1, j]:\n",
    "        count_equal_backward += 1\n",
    "        if i - 2 >= 0 and r1 == board_num[i - 2, j]:\n",
    "            count_equal_backward += 1\n",
    "    \n",
    "    if flag_print:\n",
    "        print(count_equal_backward + count_equal_forward)\n",
    "    \n",
    "    if count_equal_backward + count_equal_forward >= 3:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def swap_col(board_num, row, col):\n",
    "    tmp = board_num[row, col + 1]\n",
    "    board_num[row, col + 1] = board_num[row, col]\n",
    "    board_num[row, col] = tmp\n",
    "    return board_num\n",
    "\n",
    "def swap_row(board_num, row, col):\n",
    "    tmp = board_num[row + 1, col]\n",
    "    board_num[row + 1, col] = board_num[row, col]\n",
    "    board_num[row, col] = tmp\n",
    "    return board_num\n",
    "\n",
    "def get_state(board, moves_left):\n",
    "    \n",
    "    #print(board)\n",
    "    \n",
    "    #state = np.zeros(len(board) - (N_Rows-1) + 1)\n",
    "    #\n",
    "    #c_state_index = 0\n",
    "    #for s in range(0,len(board)):\n",
    "    #    if board[s] != '\\n':\n",
    "    #        state[c_state_index] = ord(board[s]) - ord('a')\n",
    "    #        state[c_state_index] = -1 if state[c_state_index] < 0 else state[c_state_index]\n",
    "    #        \n",
    "    #        state[c_state_index] = (state[c_state_index] + 1) / 5.0\n",
    "    #        c_state_index += 1\n",
    "    #        \n",
    "    #state[-1] = moves_left / 25.0\n",
    "    \n",
    "    board_num = np.zeros((N_Rows, N_Cols))\n",
    "    col = 0\n",
    "    row = 0\n",
    "    for s in range(0, len(board)):\n",
    "        if board[s] != '\\n':\n",
    "            board_num[row, col] = ord(board[s]) - ord('a')\n",
    "            board_num[row, col] = -1 if board_num[row, col] < 0 else board_num[row, col]\n",
    "            \n",
    "            col += 1\n",
    "        else:\n",
    "            col = 0\n",
    "            row += 1\n",
    "    \n",
    "    print(board_num)\n",
    "    flag_print = False\n",
    "    col_feature = np.zeros((N_Rows, N_Cols))\n",
    "    for row in range(0, N_Rows):\n",
    "        for col in range(0, N_Cols - 1):\n",
    "            #flag_print = False\n",
    "            #if row == 3 and col == 3:\n",
    "            #    flag_print = True\n",
    "            \n",
    "            board_num = swap_col(board_num, row, col)\n",
    "            if flag_print:\n",
    "                print(board_num)\n",
    "            if (match_in_row_from_index(board_num, row, col, flag_print) or\n",
    "                match_in_col_from_index(board_num, row, col, flag_print) or\n",
    "                match_in_row_from_index(board_num, row, col + 1, flag_print) or\n",
    "                match_in_col_from_index(board_num, row, col + 1, flag_print)):\n",
    "                col_feature[row, col] = 1\n",
    "            board_num = swap_col(board_num, row, col)\n",
    "            if flag_print:\n",
    "                print(board_num)\n",
    "    #print(col_feature)\n",
    "    \n",
    "    row_feature = np.zeros((N_Rows, N_Cols))\n",
    "    for row in range(0, N_Rows - 1):\n",
    "        for col in range(0, N_Cols):\n",
    "            #flag_print = False\n",
    "            #if row == 3 and col == 3:\n",
    "            #    flag_print = True\n",
    "            \n",
    "            board_num = swap_row(board_num, row, col)\n",
    "            if flag_print:\n",
    "                print(board_num)\n",
    "            if (match_in_row_from_index(board_num, row, col, flag_print) or\n",
    "                match_in_col_from_index(board_num, row, col, flag_print) or\n",
    "                match_in_row_from_index(board_num, row + 1, col, flag_print) or\n",
    "                match_in_col_from_index(board_num, row + 1, col, flag_print)):\n",
    "                row_feature[row, col] = 1\n",
    "            board_num = swap_row(board_num, row, col)\n",
    "            if flag_print:\n",
    "                print(board_num)\n",
    "    print(row_feature)\n",
    "    return board_num\n",
    "\n",
    "def get_action_from(move):\n",
    "    action = np.array(move)\n",
    "    \n",
    "    if move[2]:\n",
    "        action[2] = 1\n",
    "    else:\n",
    "        action[2] = 0\n",
    "    \n",
    "    out_action = (action[2]) * (N_Rows * N_Cols) + (action[0] * N_Rows + action[1])\n",
    "    \n",
    "    return out_action\n",
    "\n",
    "def get_move_from(action):\n",
    "    row_col = action % (N_Rows * N_Cols)\n",
    "    \n",
    "    dir = int(action / (N_Rows * N_Cols))\n",
    "    \n",
    "    return (int(row_col / N_Rows), row_col % N_Rows, dir >= 1)\n",
    "\n",
    "def get_valid_actions():\n",
    "    All_Actions = np.arange(0,N_Rows*N_Cols*N_Dir)\n",
    "    print(All_Actions)\n",
    "    \n",
    "    remove_ids = []\n",
    "    for i in range(0, N_Rows):\n",
    "        action_id = get_action_from((N_Cols - 1, i, False))\n",
    "        remove_ids.append(action_id)\n",
    "    \n",
    "    for i in range(0, N_Cols):\n",
    "        action_id = get_action_from((i, N_Rows - 1, True))\n",
    "        remove_ids.append(action_id)\n",
    "    print(remove_ids)\n",
    "    return np.delete(All_Actions, remove_ids, axis=0), remove_ids\n",
    "\n",
    "All_Actions, remove_ids = get_valid_actions()\n",
    "print(All_Actions)\n",
    "\n",
    "N_All_Actions = len(All_Actions)\n",
    "N_State = N_Rows * N_Cols + 1 # board game state + moves_left\n",
    "\n",
    "###################################################### test ############################################\n",
    "\n",
    "board_sample_str = \"da#bb#ac\\n#bbccbbd\\n#cd#a#d#\\n#c#d#ddc\\n##ba##bc\\nacadc#c#\\n#d##cc##\\nc#cbdacd\\ndca#d#b#\\ndd#dccdb\"\n",
    "\n",
    "get_state(board_sample_str, 0)\n",
    "\n",
    "# test action conversion\n",
    "num_error_in_conversion = 0\n",
    "for i in range(0,160):\n",
    "    a = get_move_from(i)\n",
    "    ii = get_action_from(a)\n",
    "    if i != ii:\n",
    "        num_error_in_conversion += 1\n",
    "print(\"number of errors happens in action conversion: \", num_error_in_conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 9, True)\n",
      "(7, 3, False)\n"
     ]
    }
   ],
   "source": [
    "print(get_move_from(159))\n",
    "print(get_move_from(73))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-52c87ee0a700>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdqn_agent\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAgent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mN_State\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mN_All_Actions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mdeep_q_learning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Kourosh\\Project\\Ubisoft\\RedLynx_ML_Assignment\\dqn_agent.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, state_size, action_size, seed)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m# Q-Network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqnetwork_local\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqnetwork_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqnetwork_local\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    424\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    222\u001b[0m                 \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m                     \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from dqn_agent import Agent\n",
    "\n",
    "agent = Agent(state_size=N_State, action_size=N_All_Actions, seed=0)\n",
    "\n",
    "class deep_q_learning():\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    def __init__(self, eps_start=1.0, eps_end=0.01, eps_decay=0.9998):\n",
    "        self.eps_start=eps_start\n",
    "        self.eps_end=eps_end\n",
    "        self.eps_decay=eps_decay\n",
    "        \n",
    "        self.eps = self.eps_start\n",
    "        self.scores_window = deque(maxlen=100)\n",
    "        self.score = 0\n",
    "        self.cur_episode = 0\n",
    "        \n",
    "        self.load()\n",
    "        \n",
    "    def update(self, board, move, score_delta, next_board, moves_left):\n",
    "        state = get_state(board, moves_left + 1)\n",
    "        reward = score_delta / 100.0\n",
    "        action = get_action_from(move)\n",
    "        next_state = get_state(next_board, moves_left)\n",
    "        done = (moves_left == 0)\n",
    "        \n",
    "        self.mean_loss = agent.step(state, action, reward, next_state, done)\n",
    "        \n",
    "        self.score += reward\n",
    "        return\n",
    "    \n",
    "    def predict_action(self, board, score, moves_left):\n",
    "        state = get_state(board, moves_left)\n",
    "        action = agent.act(state, self.eps)\n",
    "        \n",
    "        return get_move_from(All_Actions[action])\n",
    "    \n",
    "    def init_episode(self):\n",
    "        self.scores_window.append(self.score)  # save most recent score\n",
    "        self.eps = max(self.eps_end, self.eps_decay*self.eps) # decrease epsilon\n",
    "        \n",
    "        print('\\rEpisode {}\\t Average Score: {:.3f}\\t \\\n",
    "              Average Loss: {:.5f}, \\t Epsilon: {:.5f}'.format( \\\n",
    "                                                               self.cur_episode,\n",
    "                                                               np.mean(self.scores_window), \n",
    "                                                               self.mean_loss,\n",
    "                                                               self.eps), end=\"\")\n",
    "        if self.cur_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\t Average Score: {:.3f}\\t \\\n",
    "              Average Loss: {:.5f}, \\t Epsilon: {:.5f}'.format( \\\n",
    "                                                               self.cur_episode,\n",
    "                                                               np.mean(self.scores_window), \n",
    "                                                               self.mean_loss,\n",
    "                                                               self.eps))\n",
    "            self.save()\n",
    "        \n",
    "        self.score = 0\n",
    "        self.cur_episode += 1\n",
    "        \n",
    "    def save(self):\n",
    "        torch.save({\n",
    "            'episode': self.cur_episode,\n",
    "            'model_state_dict': agent.qnetwork_local.state_dict(),\n",
    "            'optimizer_state_dict': agent.optimizer.state_dict()}, 'checkpoint.pth')\n",
    "        \n",
    "    def load(self):\n",
    "        if os.path.exists('checkpoint.pth'):\n",
    "            checkpoint = torch.load('checkpoint.pth')\n",
    "            agent.qnetwork_local.load_state_dict(checkpoint['model_state_dict'])\n",
    "            agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            self.cur_episode = checkpoint['episode']\n",
    "            agent.qnetwork_local.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 302934307671667531413257853548643485645\n",
      "da#bb#ac\n",
      "#bbccbbd\n",
      "#cd#a#d#\n",
      "#c#d#ddc\n",
      "##ba##bc\n",
      "acadc#c#\n",
      "#d##cc##\n",
      "c#cbdacd\n",
      "dca#d#b#\n",
      "dd#dccdb\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 7 is out of bounds for axis 1 with size 7",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-b9b1c46c42a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mspeedup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraphical\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mai_callback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransition_callback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_of_game_callback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspeedup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Kourosh\\Project\\Ubisoft\\RedLynx_ML_Assignment\\graphical.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_enter_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mST_POSTGAME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ask_ai\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mST_ANIMATING\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_animate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Kourosh\\Project\\Ubisoft\\RedLynx_ML_Assignment\\graphical.py\u001b[0m in \u001b[0;36m_ask_ai\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_ask_ai\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[0mbefore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m         \u001b[0mmove\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mai_callback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbefore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves_left\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[0mnxt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msdif\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmove\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboard_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnxt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-b9b1c46c42a2>\u001b[0m in \u001b[0;36mai_callback\u001b[1;34m(board, score, moves_left)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0mmy_dqn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mpredicted_move\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_dqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmoves_left\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m#dir = random.randint(0, 1) == 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-52c87ee0a700>\u001b[0m in \u001b[0;36mpredict_action\u001b[1;34m(self, board, score, moves_left)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mboard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmoves_left\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmoves_left\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-39-f3de2b9516fb>\u001b[0m in \u001b[0;36mget_state\u001b[1;34m(board, moves_left)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mboard\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0mboard_num\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m             \u001b[0mboard_num\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mboard_num\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mboard_num\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 7 is out of bounds for axis 1 with size 7"
     ]
    }
   ],
   "source": [
    "global my_dqn\n",
    "\n",
    "def ai_callback(board, score, moves_left):\n",
    "    global my_dqn\n",
    "    \n",
    "    predicted_move = my_dqn.predict_action(board, score, moves_left)\n",
    "    \n",
    "    #dir = random.randint(0, 1) == 0\n",
    "    #return (random.randint(0, 7 if dir else 6), random.randint(0, 8 if dir else 9), dir)\n",
    "    return predicted_move\n",
    "\n",
    "def transition_callback(board, move, score_delta, next_board, moves_left):\n",
    "    global my_dqn\n",
    "    \n",
    "    #act_id = get_action_from(move)\n",
    "    \n",
    "    #print(act_id)\n",
    "    my_dqn.update(board, move, score_delta, next_board, moves_left)\n",
    "    #try:\n",
    "    #    exist_value = remove_ids.index(act_id)\n",
    "    #    \n",
    "    #except:\n",
    "    #    pass\n",
    "\n",
    "def end_of_game_callback(boards, scores, moves, final_score):\n",
    "    global my_dqn\n",
    "    \n",
    "    my_dqn.init_episode()\n",
    "    \n",
    "    return True # True = play another, False = Done\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    global my_dqn\n",
    "\n",
    "    # Where we save our checkpoints and graphs\n",
    "    experiment_dir = os.path.abspath(\"./experiments/{}\".format(\"ubisoft-game\"))\n",
    "    \n",
    "    my_dqn = deep_q_learning()\n",
    "\n",
    "    speedup = 1.0\n",
    "    g = graphical.Game(ai_callback, transition_callback, end_of_game_callback, speedup)\n",
    "    g.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork():\n",
    "    \"\"\"Q-Value Estimator neural network.\n",
    "\n",
    "    This network is used for both the Q-Network and the Target Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scope=\"estimator\", summaries_dir=None, _learning_rate=5e-5):\n",
    "        self.scope = scope\n",
    "        # Writes Tensorboard summaries to disk\n",
    "        self.summary_writer = None\n",
    "        with tf.variable_scope(scope):\n",
    "            # Build the graph\n",
    "            self._build_model()\n",
    "            if summaries_dir:\n",
    "                summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n",
    "                if not os.path.exists(summary_dir):\n",
    "                    os.makedirs(summary_dir)\n",
    "                self.summary_writer = tf.summary.FileWriter(summary_dir)\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Builds the Tensorflow graph.\n",
    "        \"\"\"\n",
    "\n",
    "        # Placeholders for our input\n",
    "        # Our inputs are board game state with shape of (None, N_State)\n",
    "        self.X_pl = tf.placeholder(shape=[None, N_State], dtype=tf.float32, name=\"X\")\n",
    "        # The TD target value\n",
    "        self.y_pl = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
    "        # Integer id of which action was selected\n",
    "        self.actions_pl = tf.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n",
    "\n",
    "        X = self.X_pl\n",
    "        batch_size = tf.shape(self.X_pl)[0]\n",
    "\n",
    "        # Three fully connected layers\n",
    "        fully1 = tf.contrib.layers.fully_connected(X, 100, activation_fn=tf.nn.relu)      # 80 to 100\n",
    "        fully2 = tf.contrib.layers.fully_connected(fully1, 120, activation_fn=tf.nn.relu) # 100 to 120\n",
    "        fully3 = tf.contrib.layers.fully_connected(fully2, 140, activation_fn=tf.nn.relu) # 120 to 140\n",
    "\n",
    "        # output layers\n",
    "        self.predictions = tf.contrib.layers.fully_connected(fully3, N_All_Actions, activation_fn=None)  # 140 to 160\n",
    "\n",
    "        # Get the predictions for the chosen actions only\n",
    "        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.actions_pl\n",
    "        self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n",
    "\n",
    "        # Calculate the loss\n",
    "        self.losses = tf.squared_difference(self.y_pl, self.action_predictions)\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "\n",
    "        # Optimizer Parameters from original paper\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
    "        self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "\n",
    "        # Summaries for Tensorboard\n",
    "        #self.summaries = tf.summary.merge([\n",
    "        #    tf.summary.scalar(\"loss\", self.loss),\n",
    "        #    tf.summary.histogram(\"loss_hist\", self.losses),\n",
    "        #    tf.summary.histogram(\"q_values_hist\", self.predictions),\n",
    "        #    tf.summary.scalar(\"max_q_value\", tf.reduce_max(self.predictions))\n",
    "        #])\n",
    "\n",
    "    def predict(self, sess, s):\n",
    "        \"\"\"\n",
    "        Predicts action values.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session\n",
    "          s: State input of shape [batch_size, N_State]\n",
    "\n",
    "        Returns:\n",
    "          Tensor of shape [batch_size, N_All_Actions] containing the estimated \n",
    "          action values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.predictions, { self.X_pl: s })\n",
    "\n",
    "    def update(self, sess, s, a, y, episode_num):\n",
    "        \"\"\"\n",
    "        Updates the estimator towards the given targets.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session object\n",
    "          s: State input of shape [batch_size, N_State]\n",
    "          a: Chosen actions of shape [batch_size]\n",
    "          y: Targets of shape [batch_size]\n",
    "\n",
    "        Returns:\n",
    "          The calculated loss on the batch.\n",
    "        \"\"\"\n",
    "        feed_dict = { self.X_pl: s, self.y_pl: y, self.actions_pl: a }\n",
    "        global_step, _, loss = sess.run(\n",
    "            [tf.contrib.framework.get_global_step(), self.train_op, self.loss],\n",
    "            feed_dict)\n",
    "        #if self.summary_writer and episode_num % report_frequency == 0:\n",
    "        #    self.summary_writer.add_summary(summaries, global_step)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetworkCopier():\n",
    "    \"\"\"\n",
    "    Copy model parameters of one estimator to another.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, estimator1, estimator2):\n",
    "        \"\"\"\n",
    "        Defines copy-work operation graph.  \n",
    "        Args:\n",
    "          estimator1: Estimator to copy the paramters from\n",
    "          estimator2: Estimator to copy the parameters to\n",
    "        \"\"\"\n",
    "        e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)]\n",
    "        e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "        e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)]\n",
    "        e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "\n",
    "        self.update_ops = []\n",
    "        for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "            op = e2_v.assign(e1_v)\n",
    "            self.update_ops.append(op)\n",
    "            \n",
    "    def make(self, sess):\n",
    "        \"\"\"\n",
    "        Makes copy.\n",
    "        Args:\n",
    "            sess: Tensorflow session instance\n",
    "        \"\"\"\n",
    "        sess.run(self.update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "\n",
    "    Args:\n",
    "        estimator: An estimator that returns q values for a given state\n",
    "        nA: Number of actions in the environment.\n",
    "\n",
    "    Returns:\n",
    "        A function that takes the (sess, observation, epsilon) as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "\n",
    "    \"\"\"\n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "class deep_q_learning():\n",
    "    def __init__(self, sess,\n",
    "                 q_estimator,\n",
    "                 target_estimator,\n",
    "                 num_episodes,\n",
    "                 experiment_dir,\n",
    "                 replay_memory_size=500000,\n",
    "                 replay_memory_init_size=50000,\n",
    "                 update_target_estimator_every=10000,\n",
    "                 discount_factor=0.99,\n",
    "                 epsilon_start=1.0,\n",
    "                 epsilon_end=0.1,\n",
    "                 epsilon_decay_steps=500000,\n",
    "                 batch_size=32):\n",
    "        \"\"\"\n",
    "        Q-Learning algorithm for off-policy TD control using Function Approximation.\n",
    "        Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "\n",
    "        Args:\n",
    "            sess: Tensorflow Session object\n",
    "            q_estimator: Estimator object used for the q values\n",
    "            target_estimator: Estimator object used for the targets\n",
    "            num_episodes: Number of episodes to run for\n",
    "            experiment_dir: Directory to save Tensorflow summaries in\n",
    "            replay_memory_size: Size of the replay memory\n",
    "            replay_memory_init_size: Number of random experiences to sampel when initializing \n",
    "                                     the reply memory.\n",
    "            update_target_estimator_every: Copy parameters from the Q estimator to the \n",
    "                                           target estimator every N steps\n",
    "            discount_factor: Gamma discount factor\n",
    "            epsilon_start: Chance to sample a random action when taking an action.\n",
    "                           Epsilon is decayed over time and this is the start value\n",
    "            epsilon_end: The final minimum value of epsilon after decaying is done\n",
    "            epsilon_decay_steps: Number of steps to decay epsilon over\n",
    "            batch_size: Size of batches to sample from the replay memory\n",
    "        \"\"\"\n",
    "        \n",
    "        self.sess = sess\n",
    "        self.q_estimator = q_estimator\n",
    "        self.target_estimator = target_estimator\n",
    "        self.num_episodes = num_episodes\n",
    "        self.experiment_dir = experiment_dir\n",
    "        self.replay_memory_size = replay_memory_size\n",
    "        self.replay_memory_init_size = replay_memory_init_size\n",
    "        self.update_target_estimator_every = update_target_estimator_every\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay_steps = epsilon_decay_steps\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.cur_episode = 0\n",
    "        self.cur_t = 0\n",
    "        self.loss = None\n",
    "        \n",
    "        # The replay memory\n",
    "        self.replay_memory = []\n",
    "        \n",
    "        # Make model copier object\n",
    "        self.estimator_copy = QNetworkCopier(self.q_estimator, self.target_estimator)\n",
    "\n",
    "        # Keeps track of useful statistics\n",
    "        self.stats = {'q_net_loss':0, 'episode_rewards':0, 'epsilon':0, 'counter_observation':0, 'counter_episode':0}\n",
    "\n",
    "        # Create directories for checkpoints and summaries\n",
    "        self.checkpoint_dir = os.path.join(self.experiment_dir, \"checkpoints\")\n",
    "        self.checkpoint_path = os.path.join(self.checkpoint_dir, \"model\")\n",
    "\n",
    "        if not os.path.exists(self.checkpoint_dir):\n",
    "            os.makedirs(self.checkpoint_dir)\n",
    "\n",
    "        self.saver = tf.train.Saver()\n",
    "        # Load a previous checkpoint if we find one\n",
    "        latest_checkpoint = tf.train.latest_checkpoint(self.checkpoint_dir)\n",
    "        if latest_checkpoint:\n",
    "            print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "            self.saver.restore(self.sess, latest_checkpoint)\n",
    "\n",
    "        # Get the current time step\n",
    "        self.total_t = self.sess.run(tf.contrib.framework.get_global_step())\n",
    "\n",
    "        # The epsilon decay schedule\n",
    "        self.epsilons = np.linspace(self.epsilon_start, self.epsilon_end, self.epsilon_decay_steps)\n",
    "\n",
    "        # The policy we're following\n",
    "        self.policy = make_epsilon_greedy_policy(self.q_estimator, N_All_Actions)\n",
    "    \n",
    "    def collect_observation(self, board, move, score_delta, next_board, moves_left):\n",
    "        state = get_state(board, moves_left + 1)\n",
    "        reward = score_delta / 100.0\n",
    "        action = get_action_from(move)\n",
    "        n_state = get_state(next_board, moves_left)\n",
    "        done = (moves_left == 0)\n",
    "        \n",
    "        # If our replay memory is full, pop the first element\n",
    "        if len(self.replay_memory) == self.replay_memory_size:\n",
    "            self.replay_memory.pop(0)\n",
    "        \n",
    "        self.replay_memory.append(Transition(state, action, reward, n_state, done))\n",
    "        \n",
    "        if len(self.replay_memory) < self.replay_memory_init_size:\n",
    "            return\n",
    "        \n",
    "        # Update statistics\n",
    "        if self.loss is not None:\n",
    "            self.stats['counter_observation'] += 1\n",
    "            self.stats['epsilon'] += self.epsilons[min(self.total_t, self.epsilon_decay_steps-1)]\n",
    "            self.stats['episode_rewards'] += reward\n",
    "            self.stats['q_net_loss'] += self.loss\n",
    "        \n",
    "        self.cur_t += 1\n",
    "        return\n",
    "    \n",
    "    def predict_action(self, board, score, moves_left):\n",
    "        state = get_state(board, moves_left)\n",
    "        action_probs = self.policy(self.sess, state, self.epsilons[min(self.total_t, self.epsilon_decay_steps-1)])\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        \n",
    "        return get_move_from(All_Actions[action])\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.replay_memory) < self.replay_memory_init_size:\n",
    "            return\n",
    "        \n",
    "        # Sample a minibatch from the replay memory\n",
    "        samples = random.sample(self.replay_memory, self.batch_size)\n",
    "        states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n",
    "\n",
    "        # Calculate q values and targets\n",
    "        q_values_next = target_estimator.predict(self.sess, next_states_batch)\n",
    "        targets_batch = reward_batch \\\n",
    "                      + np.invert(done_batch).astype(np.float32) \\\n",
    "                      * self.discount_factor \\\n",
    "                      * np.amax(q_values_next, axis=1)\n",
    "\n",
    "        # Perform gradient descent update\n",
    "        states_batch = np.array(states_batch)\n",
    "        self.loss = q_estimator.update(sess, states_batch, action_batch, targets_batch, self.cur_episode)\n",
    "\n",
    "        self.total_t += 1\n",
    "        \n",
    "        # Maybe update the target estimator\n",
    "        if self.total_t % self.update_target_estimator_every == 0:\n",
    "            self.estimator_copy.make(self.sess)\n",
    "            print(\"\\nCopied model parameters to target network.\")\n",
    "\n",
    "        # Print out which step we're on, useful for debugging.\n",
    "        print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n",
    "                self.cur_t, self.total_t, self.cur_episode + 1, self.num_episodes, self.loss), end=\"\")\n",
    "    \n",
    "    def init_episode(self):\n",
    "        self.cur_t = 0\n",
    "        \n",
    "        if len(self.replay_memory) < self.replay_memory_init_size:\n",
    "            return\n",
    "        # Save the current checkpoint\n",
    "        self.saver.save(self.sess, self.checkpoint_path)\n",
    "        \n",
    "        # Add summaries to tensorboard\n",
    "        if self.stats['counter_observation'] > 1000:\n",
    "            episode_summary = tf.Summary()\n",
    "            episode_summary.value.add(\n",
    "                simple_value=self.stats['epsilon'] / self.stats['counter_observation'], tag=\"episode/epsilon\")\n",
    "            episode_summary.value.add(\n",
    "                simple_value=self.stats['episode_rewards'] / self.stats['counter_episode'], tag=\"episode/reward\")\n",
    "            episode_summary.value.add(\n",
    "                simple_value=self.stats['q_net_loss'] / self.stats['counter_observation'], tag=\"QNetLoss\")\n",
    "            q_estimator.summary_writer.add_summary(episode_summary, self.cur_episode)\n",
    "            q_estimator.summary_writer.flush()\n",
    "            \n",
    "            self.stats['counter_observation'] = 0\n",
    "            self.stats['counter_episode'] = 0\n",
    "            self.stats['epsilon'] = 0\n",
    "            self.stats['episode_rewards'] = 0\n",
    "            self.stats['q_net_loss'] = 0\n",
    "        \n",
    "        # Reset\n",
    "        self.cur_episode += 1\n",
    "        self.stats['counter_episode'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global my_dqn\n",
    "\n",
    "def ai_callback(board, score, moves_left):\n",
    "    global my_dqn\n",
    "    \n",
    "    predicted_move = my_dqn.predict_action(board, score, moves_left)\n",
    "    #print(predicted_move)\n",
    "    \n",
    "    #dir = random.randint(0, 1) == 0\n",
    "    #return (random.randint(0, 7 if dir else 6), random.randint(0, 8 if dir else 9), dir)\n",
    "    return predicted_move\n",
    "\n",
    "def transition_callback(board, move, score_delta, next_board, moves_left):\n",
    "    global my_dqn\n",
    "    \n",
    "    my_dqn.collect_observation(board, move, score_delta, next_board, moves_left)\n",
    "    my_dqn.update()\n",
    "    \n",
    "    pass # This can be used to monitor outcomes of moves\n",
    "\n",
    "def end_of_game_callback(boards, scores, moves, final_score):\n",
    "    global my_dqn\n",
    "    \n",
    "    my_dqn.init_episode()\n",
    "    \n",
    "    return True # True = play another, False = Done\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    global my_dqn\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Where we save our checkpoints and graphs\n",
    "    experiment_dir = os.path.abspath(\"./experiments/{}\".format(\"ubisoft-game\"))\n",
    "\n",
    "    # Create a glboal step variable\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "    # Create estimators\n",
    "    q_estimator = QNetwork(scope=\"q_estimator\", summaries_dir=experiment_dir)\n",
    "    target_estimator = QNetwork(scope=\"target_q\")\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    my_dqn = deep_q_learning(sess,\n",
    "                             q_estimator=q_estimator,\n",
    "                             target_estimator=target_estimator,\n",
    "                             experiment_dir=experiment_dir,\n",
    "                             num_episodes=10000,\n",
    "                             replay_memory_size=500000,\n",
    "                             replay_memory_init_size=50000,\n",
    "                             update_target_estimator_every=10000,\n",
    "                             epsilon_start=1.0,\n",
    "                             epsilon_end=0.1,\n",
    "                             epsilon_decay_steps=500000,\n",
    "                             discount_factor=0.99,\n",
    "                             batch_size=32)\n",
    "\n",
    "    speedup = 1000.0\n",
    "    g = graphical.Game(ai_callback, transition_callback, end_of_game_callback, speedup)\n",
    "    g.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
