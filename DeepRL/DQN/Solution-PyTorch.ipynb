{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "#import tensorflow as tf\n",
    "import random\n",
    "import graphical, game\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed actions: [70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 89, 99, 109, 119, 129, 139, 149, 159]\n",
      "number of errors happens in action conversion:  0\n",
      "removed for example: (7, 9, True) (7, 3, False)\n",
      "Num of state:  241 , Num of actions:  142\n"
     ]
    }
   ],
   "source": [
    "N_Rows = 10\n",
    "N_Cols = 8\n",
    "N_Dir = 2\n",
    "\n",
    "def match_in_row_from_index(board_num, i, j, flag_print):\n",
    "    count_equal_forward = 1\n",
    "    count_equal_backward = 0\n",
    "    c1 = board_num[i, j]\n",
    "    if c1 < 0:\n",
    "        return False\n",
    "    \n",
    "    if j + 1 < N_Cols and c1 == board_num[i, j + 1]:\n",
    "        count_equal_forward += 1\n",
    "        if j + 2 < N_Cols and c1 == board_num[i, j + 2]:\n",
    "            count_equal_forward += 1\n",
    "    \n",
    "    if j - 1 >= 0 and c1 == board_num[i, j - 1]:\n",
    "        count_equal_backward += 1\n",
    "        if j - 2 >= 0 and c1 == board_num[i, j - 2]:\n",
    "            count_equal_backward += 1\n",
    "    if flag_print:\n",
    "        print(count_equal_backward + count_equal_forward)\n",
    "    \n",
    "    if count_equal_backward + count_equal_forward >= 3:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def match_in_col_from_index(board_num, i, j, flag_print):\n",
    "    count_equal_forward = 1\n",
    "    count_equal_backward = 0\n",
    "    r1 = board_num[i, j]\n",
    "    if r1 < 0:\n",
    "        return False\n",
    "    \n",
    "    if i + 1 < N_Rows and r1 == board_num[i + 1, j]:\n",
    "        count_equal_forward += 1\n",
    "        if i + 2 < N_Rows and r1 == board_num[i + 2, j]:\n",
    "            count_equal_forward += 1\n",
    "    \n",
    "    if i - 1 >= 0 and r1 == board_num[i - 1, j]:\n",
    "        count_equal_backward += 1\n",
    "        if i - 2 >= 0 and r1 == board_num[i - 2, j]:\n",
    "            count_equal_backward += 1\n",
    "    \n",
    "    if flag_print:\n",
    "        print(count_equal_backward + count_equal_forward)\n",
    "    \n",
    "    if count_equal_backward + count_equal_forward >= 3:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def swap_col(board_num, row, col):\n",
    "    tmp = board_num[row, col + 1]\n",
    "    board_num[row, col + 1] = board_num[row, col]\n",
    "    board_num[row, col] = tmp\n",
    "    return board_num\n",
    "\n",
    "def swap_row(board_num, row, col):\n",
    "    tmp = board_num[row + 1, col]\n",
    "    board_num[row + 1, col] = board_num[row, col]\n",
    "    board_num[row, col] = tmp\n",
    "    return board_num\n",
    "\n",
    "def get_state(board, moves_left):\n",
    "    #print(board)\n",
    "    \n",
    "    board_num = np.zeros((N_Rows, N_Cols))\n",
    "    col = 0\n",
    "    row = 0\n",
    "    for s in range(0, len(board)):\n",
    "        if board[s] != '\\n':\n",
    "            board_num[row, col] = ord(board[s]) - ord('a')\n",
    "            board_num[row, col] = -1 if board_num[row, col] < 0 else board_num[row, col]\n",
    "            \n",
    "            col += 1\n",
    "        else:\n",
    "            col = 0\n",
    "            row += 1\n",
    "    \n",
    "    #print(board_num)\n",
    "    flag_print = False\n",
    "    col_feature = np.zeros((N_Rows, N_Cols))\n",
    "    for row in range(0, N_Rows):\n",
    "        for col in range(0, N_Cols - 1):\n",
    "            #flag_print = False\n",
    "            #if row == 3 and col == 3:\n",
    "            #    flag_print = True\n",
    "            \n",
    "            board_num = swap_col(board_num, row, col)\n",
    "            if flag_print:\n",
    "                print(board_num)\n",
    "            if (match_in_row_from_index(board_num, row, col, flag_print) or\n",
    "                match_in_col_from_index(board_num, row, col, flag_print) or\n",
    "                match_in_row_from_index(board_num, row, col + 1, flag_print) or\n",
    "                match_in_col_from_index(board_num, row, col + 1, flag_print)):\n",
    "                col_feature[row, col] = 1\n",
    "            board_num = swap_col(board_num, row, col)\n",
    "            if flag_print:\n",
    "                print(board_num)\n",
    "    #print(col_feature)\n",
    "    \n",
    "    row_feature = np.zeros((N_Rows, N_Cols))\n",
    "    for row in range(0, N_Rows - 1):\n",
    "        for col in range(0, N_Cols):\n",
    "            #flag_print = False\n",
    "            #if row == 3 and col == 3:\n",
    "            #    flag_print = True\n",
    "            \n",
    "            board_num = swap_row(board_num, row, col)\n",
    "            if flag_print:\n",
    "                print(board_num)\n",
    "            if (match_in_row_from_index(board_num, row, col, flag_print) or\n",
    "                match_in_col_from_index(board_num, row, col, flag_print) or\n",
    "                match_in_row_from_index(board_num, row + 1, col, flag_print) or\n",
    "                match_in_col_from_index(board_num, row + 1, col, flag_print)):\n",
    "                row_feature[row, col] = 1\n",
    "            board_num = swap_row(board_num, row, col)\n",
    "            if flag_print:\n",
    "                print(board_num)\n",
    "    #print(row_feature)\n",
    "    \n",
    "    ###### put features in the state\n",
    "    state = np.zeros(3 * (N_Rows * N_Cols) + 1)\n",
    "    \n",
    "    c_state_index = 0\n",
    "    #for s in range(0,len(board)):\n",
    "    #    if board[s] != '\\n':\n",
    "    #        state[c_state_index] = ord(board[s]) - ord('a')\n",
    "    #        state[c_state_index] = -1 if state[c_state_index] < 0 else state[c_state_index]\n",
    "    #        \n",
    "    #        state[c_state_index] = (state[c_state_index] + 1) / 5.0\n",
    "    #        c_state_index += 1\n",
    "    for row in range(0, N_Rows):\n",
    "        for col in range(0, N_Cols):\n",
    "            state[c_state_index] = col_feature[row, col] + np.random.normal(0,0.01,1)\n",
    "            c_state_index += 1\n",
    "            state[c_state_index] = row_feature[row, col] + np.random.normal(0,0.01,1)\n",
    "            c_state_index += 1\n",
    "            state[c_state_index] = board_num[row, col]\n",
    "            c_state_index += 1\n",
    "\n",
    "    state[c_state_index] = moves_left / 25.0\n",
    "    \n",
    "    return state\n",
    "\n",
    "def get_action_from(move):\n",
    "    action = np.array(move)\n",
    "    \n",
    "    if move[2]:\n",
    "        action[2] = 1\n",
    "    else:\n",
    "        action[2] = 0\n",
    "    \n",
    "    out_action = (action[2]) * (N_Rows * N_Cols) + (action[0] * N_Rows + action[1])\n",
    "    \n",
    "    return out_action\n",
    "\n",
    "def get_move_from(action):\n",
    "    row_col = action % (N_Rows * N_Cols)\n",
    "    \n",
    "    dir = int(action / (N_Rows * N_Cols))\n",
    "    \n",
    "    return (int(row_col / N_Rows), row_col % N_Rows, dir >= 1)\n",
    "\n",
    "def get_valid_actions():\n",
    "    All_Actions = np.arange(0,N_Rows*N_Cols*N_Dir)\n",
    "    #print(All_Actions)\n",
    "    \n",
    "    remove_ids = []\n",
    "    for i in range(0, N_Rows):\n",
    "        action_id = get_action_from((N_Cols - 1, i, False))\n",
    "        remove_ids.append(action_id)\n",
    "    \n",
    "    for i in range(0, N_Cols):\n",
    "        action_id = get_action_from((i, N_Rows - 1, True))\n",
    "        remove_ids.append(action_id)\n",
    "    print(\"removed actions:\", remove_ids)\n",
    "    return np.delete(All_Actions, remove_ids, axis=0), remove_ids\n",
    "\n",
    "All_Actions, remove_ids = get_valid_actions()\n",
    "#print(All_Actions)\n",
    "\n",
    "board_sample_str = \"da#bb#ac\\n#bbccbbd\\n#cd#a#d#\\n#c#d#ddc\\n##ba##bc\\nacadc#c#\\n#d##cc##\\nc#cbdacd\\ndca#d#b#\\ndd#dccdb\"\n",
    "state = get_state(board_sample_str, 0)\n",
    "\n",
    "N_All_Actions = len(All_Actions)\n",
    "N_State = len(state)\n",
    "\n",
    "###################################################### test ############################################\n",
    "\n",
    "# test action conversion\n",
    "num_error_in_conversion = 0\n",
    "for i in range(0,160):\n",
    "    a = get_move_from(i)\n",
    "    ii = get_action_from(a)\n",
    "    if i != ii:\n",
    "        num_error_in_conversion += 1\n",
    "print(\"number of errors happens in action conversion: \", num_error_in_conversion)\n",
    "\n",
    "print(\"removed for example:\", get_move_from(159), get_move_from(73))\n",
    "\n",
    "print(\"Num of state: \", N_State, \", Num of actions: \", N_All_Actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from dqn_agent import Agent\n",
    "\n",
    "agent = Agent(state_size=N_State, action_size=N_All_Actions, seed=0)\n",
    "\n",
    "class deep_q_learning():\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    def __init__(self, eps_start=1.0, eps_end=0.01, eps_decay=0.9998):\n",
    "        self.eps_start=eps_start\n",
    "        self.eps_end=eps_end\n",
    "        self.eps_decay=eps_decay\n",
    "        \n",
    "        self.eps = self.eps_start\n",
    "        self.scores_window = deque(maxlen=100)\n",
    "        self.score = 0\n",
    "        self.cur_episode = 0\n",
    "        \n",
    "        self.load()\n",
    "        \n",
    "    def update(self, board, move, score_delta, next_board, moves_left):\n",
    "        state = get_state(board, moves_left + 1)\n",
    "        reward = score_delta / 100.0\n",
    "        action = get_action_from(move)\n",
    "        next_state = get_state(next_board, moves_left)\n",
    "        done = (moves_left == 0)\n",
    "        \n",
    "        self.mean_loss = agent.step(state, action, reward, next_state, done)\n",
    "        \n",
    "        self.score += reward\n",
    "        return\n",
    "    \n",
    "    def predict_action(self, board, score, moves_left):\n",
    "        state = get_state(board, moves_left)\n",
    "        action = agent.act(state, self.eps)\n",
    "        \n",
    "        return get_move_from(All_Actions[action])\n",
    "    \n",
    "    def init_episode(self):\n",
    "        self.scores_window.append(self.score)  # save most recent score\n",
    "        self.eps = max(self.eps_end, self.eps_decay*self.eps) # decrease epsilon\n",
    "        \n",
    "        print('\\rEpisode {}\\t Average Score: {:.3f}\\t \\\n",
    "              Average Loss: {:.5f}, \\t Epsilon: {:.5f}'.format( \\\n",
    "                                                               self.cur_episode,\n",
    "                                                               np.mean(self.scores_window), \n",
    "                                                               self.mean_loss,\n",
    "                                                               self.eps), end=\"\")\n",
    "        if self.cur_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\t Average Score: {:.3f}\\t \\\n",
    "              Average Loss: {:.5f}, \\t Epsilon: {:.5f}'.format( \\\n",
    "                                                               self.cur_episode,\n",
    "                                                               np.mean(self.scores_window), \n",
    "                                                               self.mean_loss,\n",
    "                                                               self.eps))\n",
    "            self.save()\n",
    "        \n",
    "        self.score = 0\n",
    "        self.cur_episode += 1\n",
    "        \n",
    "    def save(self):\n",
    "        torch.save({\n",
    "            'episode': self.cur_episode,\n",
    "            'model_state_dict': agent.qnetwork_local.state_dict(),\n",
    "            'optimizer_state_dict': agent.optimizer.state_dict()}, 'checkpoint.pth')\n",
    "        \n",
    "    def load(self):\n",
    "        if os.path.exists('checkpoint.pth'):\n",
    "            checkpoint = torch.load('checkpoint.pth')\n",
    "            agent.qnetwork_local.load_state_dict(checkpoint['model_state_dict'])\n",
    "            agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            self.cur_episode = checkpoint['episode']\n",
    "            agent.qnetwork_local.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (710) : device-side assert triggered at C:\\w\\1\\s\\tmp_conda_3.6_081743\\conda\\conda-bld\\pytorch_1572941935551\\work\\torch/csrc/generic/serialization.cpp:131",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b9b1c46c42a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mexperiment_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./experiments/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ubisoft-game\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mmy_dqn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeep_q_learning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mspeedup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-1634e96bd42b>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, eps_start, eps_end, eps_decay)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcur_episode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mboard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmove\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore_delta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_board\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmoves_left\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-1634e96bd42b>\u001b[0m in \u001b[0;36mload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'checkpoint.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m             \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'checkpoint.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqnetwork_local\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model_state_dict'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'optimizer_state_dict'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    424\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m'encoding'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m             \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    618\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdeserialized_storage_keys\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 620\u001b[1;33m         \u001b[0mdeserialized_objects\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_from_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf_should_read_directly\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    621\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moffset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m             \u001b[0moffset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cuda runtime error (710) : device-side assert triggered at C:\\w\\1\\s\\tmp_conda_3.6_081743\\conda\\conda-bld\\pytorch_1572941935551\\work\\torch/csrc/generic/serialization.cpp:131"
     ]
    }
   ],
   "source": [
    "global my_dqn\n",
    "\n",
    "def ai_callback(board, score, moves_left):\n",
    "    global my_dqn\n",
    "    \n",
    "    predicted_move = my_dqn.predict_action(board, score, moves_left)\n",
    "    \n",
    "    #dir = random.randint(0, 1) == 0\n",
    "    #return (random.randint(0, 7 if dir else 6), random.randint(0, 8 if dir else 9), dir)\n",
    "    return predicted_move\n",
    "\n",
    "def transition_callback(board, move, score_delta, next_board, moves_left):\n",
    "    global my_dqn\n",
    "    \n",
    "    #act_id = get_action_from(move)\n",
    "    \n",
    "    #print(act_id)\n",
    "    my_dqn.update(board, move, score_delta, next_board, moves_left)\n",
    "    #try:\n",
    "    #    exist_value = remove_ids.index(act_id)\n",
    "    #    \n",
    "    #except:\n",
    "    #    pass\n",
    "\n",
    "def end_of_game_callback(boards, scores, moves, final_score):\n",
    "    global my_dqn\n",
    "    \n",
    "    my_dqn.init_episode()\n",
    "    \n",
    "    return True # True = play another, False = Done\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    global my_dqn\n",
    "\n",
    "    # Where we save our checkpoints and graphs\n",
    "    experiment_dir = os.path.abspath(\"./experiments/{}\".format(\"ubisoft-game\"))\n",
    "    \n",
    "    my_dqn = deep_q_learning()\n",
    "\n",
    "    speedup = 1.0\n",
    "    g = graphical.Game(ai_callback, transition_callback, end_of_game_callback, speedup)\n",
    "    g.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork():\n",
    "    \"\"\"Q-Value Estimator neural network.\n",
    "\n",
    "    This network is used for both the Q-Network and the Target Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scope=\"estimator\", summaries_dir=None, _learning_rate=5e-5):\n",
    "        self.scope = scope\n",
    "        # Writes Tensorboard summaries to disk\n",
    "        self.summary_writer = None\n",
    "        with tf.variable_scope(scope):\n",
    "            # Build the graph\n",
    "            self._build_model()\n",
    "            if summaries_dir:\n",
    "                summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n",
    "                if not os.path.exists(summary_dir):\n",
    "                    os.makedirs(summary_dir)\n",
    "                self.summary_writer = tf.summary.FileWriter(summary_dir)\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Builds the Tensorflow graph.\n",
    "        \"\"\"\n",
    "\n",
    "        # Placeholders for our input\n",
    "        # Our inputs are board game state with shape of (None, N_State)\n",
    "        self.X_pl = tf.placeholder(shape=[None, N_State], dtype=tf.float32, name=\"X\")\n",
    "        # The TD target value\n",
    "        self.y_pl = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
    "        # Integer id of which action was selected\n",
    "        self.actions_pl = tf.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n",
    "\n",
    "        X = self.X_pl\n",
    "        batch_size = tf.shape(self.X_pl)[0]\n",
    "\n",
    "        # Three fully connected layers\n",
    "        fully1 = tf.contrib.layers.fully_connected(X, 100, activation_fn=tf.nn.relu)      # 80 to 100\n",
    "        fully2 = tf.contrib.layers.fully_connected(fully1, 120, activation_fn=tf.nn.relu) # 100 to 120\n",
    "        fully3 = tf.contrib.layers.fully_connected(fully2, 140, activation_fn=tf.nn.relu) # 120 to 140\n",
    "\n",
    "        # output layers\n",
    "        self.predictions = tf.contrib.layers.fully_connected(fully3, N_All_Actions, activation_fn=None)  # 140 to 160\n",
    "\n",
    "        # Get the predictions for the chosen actions only\n",
    "        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.actions_pl\n",
    "        self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n",
    "\n",
    "        # Calculate the loss\n",
    "        self.losses = tf.squared_difference(self.y_pl, self.action_predictions)\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "\n",
    "        # Optimizer Parameters from original paper\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
    "        self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "\n",
    "        # Summaries for Tensorboard\n",
    "        #self.summaries = tf.summary.merge([\n",
    "        #    tf.summary.scalar(\"loss\", self.loss),\n",
    "        #    tf.summary.histogram(\"loss_hist\", self.losses),\n",
    "        #    tf.summary.histogram(\"q_values_hist\", self.predictions),\n",
    "        #    tf.summary.scalar(\"max_q_value\", tf.reduce_max(self.predictions))\n",
    "        #])\n",
    "\n",
    "    def predict(self, sess, s):\n",
    "        \"\"\"\n",
    "        Predicts action values.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session\n",
    "          s: State input of shape [batch_size, N_State]\n",
    "\n",
    "        Returns:\n",
    "          Tensor of shape [batch_size, N_All_Actions] containing the estimated \n",
    "          action values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.predictions, { self.X_pl: s })\n",
    "\n",
    "    def update(self, sess, s, a, y, episode_num):\n",
    "        \"\"\"\n",
    "        Updates the estimator towards the given targets.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session object\n",
    "          s: State input of shape [batch_size, N_State]\n",
    "          a: Chosen actions of shape [batch_size]\n",
    "          y: Targets of shape [batch_size]\n",
    "\n",
    "        Returns:\n",
    "          The calculated loss on the batch.\n",
    "        \"\"\"\n",
    "        feed_dict = { self.X_pl: s, self.y_pl: y, self.actions_pl: a }\n",
    "        global_step, _, loss = sess.run(\n",
    "            [tf.contrib.framework.get_global_step(), self.train_op, self.loss],\n",
    "            feed_dict)\n",
    "        #if self.summary_writer and episode_num % report_frequency == 0:\n",
    "        #    self.summary_writer.add_summary(summaries, global_step)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetworkCopier():\n",
    "    \"\"\"\n",
    "    Copy model parameters of one estimator to another.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, estimator1, estimator2):\n",
    "        \"\"\"\n",
    "        Defines copy-work operation graph.  \n",
    "        Args:\n",
    "          estimator1: Estimator to copy the paramters from\n",
    "          estimator2: Estimator to copy the parameters to\n",
    "        \"\"\"\n",
    "        e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)]\n",
    "        e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "        e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)]\n",
    "        e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "\n",
    "        self.update_ops = []\n",
    "        for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "            op = e2_v.assign(e1_v)\n",
    "            self.update_ops.append(op)\n",
    "            \n",
    "    def make(self, sess):\n",
    "        \"\"\"\n",
    "        Makes copy.\n",
    "        Args:\n",
    "            sess: Tensorflow session instance\n",
    "        \"\"\"\n",
    "        sess.run(self.update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "\n",
    "    Args:\n",
    "        estimator: An estimator that returns q values for a given state\n",
    "        nA: Number of actions in the environment.\n",
    "\n",
    "    Returns:\n",
    "        A function that takes the (sess, observation, epsilon) as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "\n",
    "    \"\"\"\n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "class deep_q_learning():\n",
    "    def __init__(self, sess,\n",
    "                 q_estimator,\n",
    "                 target_estimator,\n",
    "                 num_episodes,\n",
    "                 experiment_dir,\n",
    "                 replay_memory_size=500000,\n",
    "                 replay_memory_init_size=50000,\n",
    "                 update_target_estimator_every=10000,\n",
    "                 discount_factor=0.99,\n",
    "                 epsilon_start=1.0,\n",
    "                 epsilon_end=0.1,\n",
    "                 epsilon_decay_steps=500000,\n",
    "                 batch_size=32):\n",
    "        \"\"\"\n",
    "        Q-Learning algorithm for off-policy TD control using Function Approximation.\n",
    "        Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "\n",
    "        Args:\n",
    "            sess: Tensorflow Session object\n",
    "            q_estimator: Estimator object used for the q values\n",
    "            target_estimator: Estimator object used for the targets\n",
    "            num_episodes: Number of episodes to run for\n",
    "            experiment_dir: Directory to save Tensorflow summaries in\n",
    "            replay_memory_size: Size of the replay memory\n",
    "            replay_memory_init_size: Number of random experiences to sampel when initializing \n",
    "                                     the reply memory.\n",
    "            update_target_estimator_every: Copy parameters from the Q estimator to the \n",
    "                                           target estimator every N steps\n",
    "            discount_factor: Gamma discount factor\n",
    "            epsilon_start: Chance to sample a random action when taking an action.\n",
    "                           Epsilon is decayed over time and this is the start value\n",
    "            epsilon_end: The final minimum value of epsilon after decaying is done\n",
    "            epsilon_decay_steps: Number of steps to decay epsilon over\n",
    "            batch_size: Size of batches to sample from the replay memory\n",
    "        \"\"\"\n",
    "        \n",
    "        self.sess = sess\n",
    "        self.q_estimator = q_estimator\n",
    "        self.target_estimator = target_estimator\n",
    "        self.num_episodes = num_episodes\n",
    "        self.experiment_dir = experiment_dir\n",
    "        self.replay_memory_size = replay_memory_size\n",
    "        self.replay_memory_init_size = replay_memory_init_size\n",
    "        self.update_target_estimator_every = update_target_estimator_every\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay_steps = epsilon_decay_steps\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.cur_episode = 0\n",
    "        self.cur_t = 0\n",
    "        self.loss = None\n",
    "        \n",
    "        # The replay memory\n",
    "        self.replay_memory = []\n",
    "        \n",
    "        # Make model copier object\n",
    "        self.estimator_copy = QNetworkCopier(self.q_estimator, self.target_estimator)\n",
    "\n",
    "        # Keeps track of useful statistics\n",
    "        self.stats = {'q_net_loss':0, 'episode_rewards':0, 'epsilon':0, 'counter_observation':0, 'counter_episode':0}\n",
    "\n",
    "        # Create directories for checkpoints and summaries\n",
    "        self.checkpoint_dir = os.path.join(self.experiment_dir, \"checkpoints\")\n",
    "        self.checkpoint_path = os.path.join(self.checkpoint_dir, \"model\")\n",
    "\n",
    "        if not os.path.exists(self.checkpoint_dir):\n",
    "            os.makedirs(self.checkpoint_dir)\n",
    "\n",
    "        self.saver = tf.train.Saver()\n",
    "        # Load a previous checkpoint if we find one\n",
    "        latest_checkpoint = tf.train.latest_checkpoint(self.checkpoint_dir)\n",
    "        if latest_checkpoint:\n",
    "            print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "            self.saver.restore(self.sess, latest_checkpoint)\n",
    "\n",
    "        # Get the current time step\n",
    "        self.total_t = self.sess.run(tf.contrib.framework.get_global_step())\n",
    "\n",
    "        # The epsilon decay schedule\n",
    "        self.epsilons = np.linspace(self.epsilon_start, self.epsilon_end, self.epsilon_decay_steps)\n",
    "\n",
    "        # The policy we're following\n",
    "        self.policy = make_epsilon_greedy_policy(self.q_estimator, N_All_Actions)\n",
    "    \n",
    "    def collect_observation(self, board, move, score_delta, next_board, moves_left):\n",
    "        state = get_state(board, moves_left + 1)\n",
    "        reward = score_delta / 100.0\n",
    "        action = get_action_from(move)\n",
    "        n_state = get_state(next_board, moves_left)\n",
    "        done = (moves_left == 0)\n",
    "        \n",
    "        # If our replay memory is full, pop the first element\n",
    "        if len(self.replay_memory) == self.replay_memory_size:\n",
    "            self.replay_memory.pop(0)\n",
    "        \n",
    "        self.replay_memory.append(Transition(state, action, reward, n_state, done))\n",
    "        \n",
    "        if len(self.replay_memory) < self.replay_memory_init_size:\n",
    "            return\n",
    "        \n",
    "        # Update statistics\n",
    "        if self.loss is not None:\n",
    "            self.stats['counter_observation'] += 1\n",
    "            self.stats['epsilon'] += self.epsilons[min(self.total_t, self.epsilon_decay_steps-1)]\n",
    "            self.stats['episode_rewards'] += reward\n",
    "            self.stats['q_net_loss'] += self.loss\n",
    "        \n",
    "        self.cur_t += 1\n",
    "        return\n",
    "    \n",
    "    def predict_action(self, board, score, moves_left):\n",
    "        state = get_state(board, moves_left)\n",
    "        action_probs = self.policy(self.sess, state, self.epsilons[min(self.total_t, self.epsilon_decay_steps-1)])\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        \n",
    "        return get_move_from(All_Actions[action])\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.replay_memory) < self.replay_memory_init_size:\n",
    "            return\n",
    "        \n",
    "        # Sample a minibatch from the replay memory\n",
    "        samples = random.sample(self.replay_memory, self.batch_size)\n",
    "        states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n",
    "\n",
    "        # Calculate q values and targets\n",
    "        q_values_next = target_estimator.predict(self.sess, next_states_batch)\n",
    "        targets_batch = reward_batch \\\n",
    "                      + np.invert(done_batch).astype(np.float32) \\\n",
    "                      * self.discount_factor \\\n",
    "                      * np.amax(q_values_next, axis=1)\n",
    "\n",
    "        # Perform gradient descent update\n",
    "        states_batch = np.array(states_batch)\n",
    "        self.loss = q_estimator.update(sess, states_batch, action_batch, targets_batch, self.cur_episode)\n",
    "\n",
    "        self.total_t += 1\n",
    "        \n",
    "        # Maybe update the target estimator\n",
    "        if self.total_t % self.update_target_estimator_every == 0:\n",
    "            self.estimator_copy.make(self.sess)\n",
    "            print(\"\\nCopied model parameters to target network.\")\n",
    "\n",
    "        # Print out which step we're on, useful for debugging.\n",
    "        print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n",
    "                self.cur_t, self.total_t, self.cur_episode + 1, self.num_episodes, self.loss), end=\"\")\n",
    "    \n",
    "    def init_episode(self):\n",
    "        self.cur_t = 0\n",
    "        \n",
    "        if len(self.replay_memory) < self.replay_memory_init_size:\n",
    "            return\n",
    "        # Save the current checkpoint\n",
    "        self.saver.save(self.sess, self.checkpoint_path)\n",
    "        \n",
    "        # Add summaries to tensorboard\n",
    "        if self.stats['counter_observation'] > 1000:\n",
    "            episode_summary = tf.Summary()\n",
    "            episode_summary.value.add(\n",
    "                simple_value=self.stats['epsilon'] / self.stats['counter_observation'], tag=\"episode/epsilon\")\n",
    "            episode_summary.value.add(\n",
    "                simple_value=self.stats['episode_rewards'] / self.stats['counter_episode'], tag=\"episode/reward\")\n",
    "            episode_summary.value.add(\n",
    "                simple_value=self.stats['q_net_loss'] / self.stats['counter_observation'], tag=\"QNetLoss\")\n",
    "            q_estimator.summary_writer.add_summary(episode_summary, self.cur_episode)\n",
    "            q_estimator.summary_writer.flush()\n",
    "            \n",
    "            self.stats['counter_observation'] = 0\n",
    "            self.stats['counter_episode'] = 0\n",
    "            self.stats['epsilon'] = 0\n",
    "            self.stats['episode_rewards'] = 0\n",
    "            self.stats['q_net_loss'] = 0\n",
    "        \n",
    "        # Reset\n",
    "        self.cur_episode += 1\n",
    "        self.stats['counter_episode'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global my_dqn\n",
    "\n",
    "def ai_callback(board, score, moves_left):\n",
    "    global my_dqn\n",
    "    \n",
    "    predicted_move = my_dqn.predict_action(board, score, moves_left)\n",
    "    #print(predicted_move)\n",
    "    \n",
    "    #dir = random.randint(0, 1) == 0\n",
    "    #return (random.randint(0, 7 if dir else 6), random.randint(0, 8 if dir else 9), dir)\n",
    "    return predicted_move\n",
    "\n",
    "def transition_callback(board, move, score_delta, next_board, moves_left):\n",
    "    global my_dqn\n",
    "    \n",
    "    my_dqn.collect_observation(board, move, score_delta, next_board, moves_left)\n",
    "    my_dqn.update()\n",
    "    \n",
    "    pass # This can be used to monitor outcomes of moves\n",
    "\n",
    "def end_of_game_callback(boards, scores, moves, final_score):\n",
    "    global my_dqn\n",
    "    \n",
    "    my_dqn.init_episode()\n",
    "    \n",
    "    return True # True = play another, False = Done\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    global my_dqn\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Where we save our checkpoints and graphs\n",
    "    experiment_dir = os.path.abspath(\"./experiments/{}\".format(\"ubisoft-game\"))\n",
    "\n",
    "    # Create a glboal step variable\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "    # Create estimators\n",
    "    q_estimator = QNetwork(scope=\"q_estimator\", summaries_dir=experiment_dir)\n",
    "    target_estimator = QNetwork(scope=\"target_q\")\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    my_dqn = deep_q_learning(sess,\n",
    "                             q_estimator=q_estimator,\n",
    "                             target_estimator=target_estimator,\n",
    "                             experiment_dir=experiment_dir,\n",
    "                             num_episodes=10000,\n",
    "                             replay_memory_size=500000,\n",
    "                             replay_memory_init_size=50000,\n",
    "                             update_target_estimator_every=10000,\n",
    "                             epsilon_start=1.0,\n",
    "                             epsilon_end=0.1,\n",
    "                             epsilon_decay_steps=500000,\n",
    "                             discount_factor=0.99,\n",
    "                             batch_size=32)\n",
    "\n",
    "    speedup = 1000.0\n",
    "    g = graphical.Game(ai_callback, transition_callback, end_of_game_callback, speedup)\n",
    "    g.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
