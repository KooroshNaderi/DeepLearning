{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import graphical, game\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed actions: [70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 89, 99, 109, 119, 129, 139, 149, 159]\n",
      "number of errors happens in action conversion:  0\n",
      "removed for example: (7, 9, True) (7, 3, False)\n",
      "Num of state:  160 , Num of actions:  142\n"
     ]
    }
   ],
   "source": [
    "N_Rows = 10\n",
    "N_Cols = 8\n",
    "N_Dir = 2\n",
    "\n",
    "def match_in_row_from_index(board_num, i, j, flag_print):\n",
    "    count_equal_forward = 1\n",
    "    count_equal_backward = 0\n",
    "    c1 = board_num[i, j]\n",
    "    if c1 < 0:\n",
    "        return False\n",
    "    \n",
    "    if j + 1 < N_Cols and c1 == board_num[i, j + 1]:\n",
    "        count_equal_forward += 1\n",
    "        if j + 2 < N_Cols and c1 == board_num[i, j + 2]:\n",
    "            count_equal_forward += 1\n",
    "    \n",
    "    if j - 1 >= 0 and c1 == board_num[i, j - 1]:\n",
    "        count_equal_backward += 1\n",
    "        if j - 2 >= 0 and c1 == board_num[i, j - 2]:\n",
    "            count_equal_backward += 1\n",
    "    if flag_print:\n",
    "        print(count_equal_backward + count_equal_forward)\n",
    "    \n",
    "    if count_equal_backward + count_equal_forward >= 3:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def match_in_col_from_index(board_num, i, j, flag_print):\n",
    "    count_equal_forward = 1\n",
    "    count_equal_backward = 0\n",
    "    r1 = board_num[i, j]\n",
    "    if r1 < 0:\n",
    "        return False\n",
    "    \n",
    "    if i + 1 < N_Rows and r1 == board_num[i + 1, j]:\n",
    "        count_equal_forward += 1\n",
    "        if i + 2 < N_Rows and r1 == board_num[i + 2, j]:\n",
    "            count_equal_forward += 1\n",
    "    \n",
    "    if i - 1 >= 0 and r1 == board_num[i - 1, j]:\n",
    "        count_equal_backward += 1\n",
    "        if i - 2 >= 0 and r1 == board_num[i - 2, j]:\n",
    "            count_equal_backward += 1\n",
    "    \n",
    "    if flag_print:\n",
    "        print(count_equal_backward + count_equal_forward)\n",
    "    \n",
    "    if count_equal_backward + count_equal_forward >= 3:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def swap_col(board_num, row, col):\n",
    "    tmp = board_num[row, col + 1]\n",
    "    board_num[row, col + 1] = board_num[row, col]\n",
    "    board_num[row, col] = tmp\n",
    "    return board_num\n",
    "\n",
    "def swap_row(board_num, row, col):\n",
    "    tmp = board_num[row + 1, col]\n",
    "    board_num[row + 1, col] = board_num[row, col]\n",
    "    board_num[row, col] = tmp\n",
    "    return board_num\n",
    "\n",
    "def get_board_num(board):\n",
    "    #print(board)\n",
    "    board_num = np.zeros((N_Rows, N_Cols))\n",
    "    col = 0\n",
    "    row = 0\n",
    "    for s in range(0, len(board)):\n",
    "        if board[s] != '\\n':\n",
    "            board_num[row, col] = ord(board[s]) - ord('a')\n",
    "            board_num[row, col] = -1 if board_num[row, col] < 0 else board_num[row, col]\n",
    "            \n",
    "            col += 1\n",
    "        else:\n",
    "            col = 0\n",
    "            row += 1\n",
    "    #print(board_num)\n",
    "    return board_num\n",
    "\n",
    "def get_state(board, moves_left):\n",
    "    board_num = get_board_num(board)\n",
    "    \n",
    "    flag_print = False\n",
    "    col_feature = np.zeros((N_Rows, N_Cols))\n",
    "    for row in range(0, N_Rows):\n",
    "        for col in range(0, N_Cols - 1):\n",
    "            #flag_print = False\n",
    "            #if row == 3 and col == 3:\n",
    "            #    flag_print = True\n",
    "            \n",
    "            board_num = swap_col(board_num, row, col)\n",
    "            if flag_print:\n",
    "                print(board_num)\n",
    "            if (match_in_row_from_index(board_num, row, col, flag_print) or\n",
    "                match_in_col_from_index(board_num, row, col, flag_print) or\n",
    "                match_in_row_from_index(board_num, row, col + 1, flag_print) or\n",
    "                match_in_col_from_index(board_num, row, col + 1, flag_print)):\n",
    "                col_feature[row, col] = 1\n",
    "            board_num = swap_col(board_num, row, col)\n",
    "            if flag_print:\n",
    "                print(board_num)\n",
    "    #print(col_feature)\n",
    "    \n",
    "    row_feature = np.zeros((N_Rows, N_Cols))\n",
    "    for row in range(0, N_Rows - 1):\n",
    "        for col in range(0, N_Cols):\n",
    "            #flag_print = False\n",
    "            #if row == 3 and col == 3:\n",
    "            #    flag_print = True\n",
    "            \n",
    "            board_num = swap_row(board_num, row, col)\n",
    "            if flag_print:\n",
    "                print(board_num)\n",
    "            if (match_in_row_from_index(board_num, row, col, flag_print) or\n",
    "                match_in_col_from_index(board_num, row, col, flag_print) or\n",
    "                match_in_row_from_index(board_num, row + 1, col, flag_print) or\n",
    "                match_in_col_from_index(board_num, row + 1, col, flag_print)):\n",
    "                row_feature[row, col] = 1\n",
    "            board_num = swap_row(board_num, row, col)\n",
    "            if flag_print:\n",
    "                print(board_num)\n",
    "    #print(row_feature)\n",
    "    \n",
    "    ###### put features in the state\n",
    "    state = np.zeros(2 * (N_Rows * N_Cols))\n",
    "    \n",
    "    c_state_index = 0\n",
    "    for row in range(0, N_Rows):\n",
    "        for col in range(0, N_Cols):\n",
    "            v = 0\n",
    "            if col_feature[row, col] and row_feature[row, col]:\n",
    "                v = 1\n",
    "            elif col_feature[row, col]:\n",
    "                v = 2\n",
    "            else:\n",
    "                v = 3\n",
    "            state[c_state_index] = 2.0 * (v / 3.0) - 1.0\n",
    "            c_state_index += 1\n",
    "            state[c_state_index] = 2.0 * ((board_num[row, col] + 1) / 5.0) - 1.0\n",
    "            c_state_index += 1\n",
    "\n",
    "    #state[c_state_index] = moves_left / 25.0\n",
    "    \n",
    "    return state\n",
    "\n",
    "def get_action_from(move):\n",
    "    action = np.array(move)\n",
    "    \n",
    "    if move[2]:\n",
    "        action[2] = 1\n",
    "    else:\n",
    "        action[2] = 0\n",
    "    \n",
    "    out_action = (action[2]) * (N_Rows * N_Cols) + (action[0] * N_Rows + action[1])\n",
    "    \n",
    "    return out_action\n",
    "\n",
    "def get_move_from(action):\n",
    "    row_col = action % (N_Rows * N_Cols)\n",
    "    \n",
    "    dir = int(action / (N_Rows * N_Cols))\n",
    "    \n",
    "    return (int(row_col / N_Rows), row_col % N_Rows, dir >= 1)\n",
    "\n",
    "def get_valid_actions():\n",
    "    All_Actions = np.arange(0,N_Rows*N_Cols*N_Dir)\n",
    "    #print(All_Actions)\n",
    "    \n",
    "    remove_ids = []\n",
    "    for i in range(0, N_Rows):\n",
    "        action_id = get_action_from((N_Cols - 1, i, False))\n",
    "        remove_ids.append(action_id)\n",
    "    \n",
    "    for i in range(0, N_Cols):\n",
    "        action_id = get_action_from((i, N_Rows - 1, True))\n",
    "        remove_ids.append(action_id)\n",
    "    print(\"removed actions:\", remove_ids)\n",
    "    return np.delete(All_Actions, remove_ids, axis=0), remove_ids\n",
    "\n",
    "All_Actions, remove_ids = get_valid_actions()\n",
    "#print(All_Actions)\n",
    "\n",
    "board_sample_str = \"da#bb#ac\\n#bbccbbd\\n#cd#a#d#\\n#c#d#ddc\\n##ba##bc\\nacadc#c#\\n#d##cc##\\nc#cbdacd\\ndca#d#b#\\ndd#dccdb\"\n",
    "state = get_state(board_sample_str, 0)\n",
    "\n",
    "N_All_Actions = len(All_Actions)\n",
    "N_State = len(state)\n",
    "\n",
    "###################################################### test ############################################\n",
    "\n",
    "# test action conversion\n",
    "num_error_in_conversion = 0\n",
    "for i in range(0,160):\n",
    "    a = get_move_from(i)\n",
    "    ii = get_action_from(a)\n",
    "    if i != ii:\n",
    "        num_error_in_conversion += 1\n",
    "print(\"number of errors happens in action conversion: \", num_error_in_conversion)\n",
    "\n",
    "print(\"removed for example:\", get_move_from(159), get_move_from(73))\n",
    "\n",
    "print(\"Num of state: \", N_State, \", Num of actions: \", N_All_Actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork():\n",
    "    \"\"\"Q-Value Estimator neural network.\n",
    "\n",
    "    This network is used for both the Q-Network and the Target Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, init_learning_rate = 0.00025, scope=\"estimator\", summaries_dir=None):\n",
    "        self.init_learning_rate = init_learning_rate\n",
    "        self.scope = scope\n",
    "        # Writes Tensorboard summaries to disk\n",
    "        self.summary_writer = None\n",
    "        with tf.variable_scope(scope):\n",
    "            # Build the graph\n",
    "            self._build_model()\n",
    "            if summaries_dir:\n",
    "                summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n",
    "                if not os.path.exists(summary_dir):\n",
    "                    os.makedirs(summary_dir)\n",
    "                self.summary_writer = tf.summary.FileWriter(summary_dir)\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Builds the Tensorflow graph.\n",
    "        \"\"\"\n",
    "\n",
    "        # Placeholders for our input\n",
    "        # Our inputs are board game state with shape of (None, N_State)\n",
    "        self.X_pl = tf.placeholder(shape=[None, N_State], dtype=tf.float32, name=\"X\")\n",
    "        # The TD target value\n",
    "        self.y_pl = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
    "        # Integer id of which action was selected\n",
    "        self.actions_pl = tf.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n",
    "\n",
    "        X = self.X_pl\n",
    "        batch_size = tf.shape(self.X_pl)[0]\n",
    "\n",
    "        # Three fully connected layers\n",
    "        fully1 = tf.layers.dense(X, 64, activation=tf.nn.relu)      \n",
    "        fully2 = tf.layers.dense(fully1, 64, activation=tf.nn.relu) \n",
    "        fully3 = tf.layers.dense(fully2, N_All_Actions, activation=tf.nn.relu) \n",
    "\n",
    "        # output layers\n",
    "        self.predictions = tf.layers.dense(fully3, N_All_Actions, activation=tf.nn.relu)\n",
    "\n",
    "        # Get the predictions for the chosen actions only\n",
    "        self.action_predictions = tf.reduce_sum(self.predictions * tf.one_hot(self.actions_pl, N_All_Actions), axis=1)\n",
    "\n",
    "        # Calculate the loss\n",
    "        self.losses = tf.squared_difference(self.y_pl, self.action_predictions)\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.init_learning_rate) # tf.train.RMSPropOptimizer(self.init_learning_rate, 0.99, 0.0, 1e-6)\n",
    "        self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "\n",
    "        # Summaries for Tensorboard\n",
    "        #self.summaries = tf.summary.merge([\n",
    "        #    tf.summary.scalar(\"loss\", self.loss),\n",
    "        #    tf.summary.histogram(\"loss_hist\", self.losses),\n",
    "        #    tf.summary.histogram(\"q_values_hist\", self.predictions),\n",
    "        #    tf.summary.scalar(\"max_q_value\", tf.reduce_max(self.predictions))\n",
    "        #])\n",
    "\n",
    "    def predict(self, sess, s):\n",
    "        \"\"\"\n",
    "        Predicts action values.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session\n",
    "          s: State input of shape [batch_size, N_State]\n",
    "\n",
    "        Returns:\n",
    "          Tensor of shape [batch_size, N_All_Actions] containing the estimated \n",
    "          action values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.predictions, { self.X_pl: s })\n",
    "\n",
    "    def update(self, sess, s, a, y):\n",
    "        \"\"\"\n",
    "        Updates the estimator towards the given targets.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session object\n",
    "          s: State input of shape [batch_size, N_State]\n",
    "          a: Chosen actions of shape [batch_size]\n",
    "          y: Targets of shape [batch_size]\n",
    "\n",
    "        Returns:\n",
    "          The calculated loss on the batch.\n",
    "        \"\"\"\n",
    "        \n",
    "        feed_dict = { self.X_pl: s, self.y_pl: y, self.actions_pl: a }\n",
    "        global_step, _, loss, pred = sess.run(\n",
    "            [tf.contrib.framework.get_global_step(), self.train_op, self.loss, self.action_predictions],#self.summaries,\n",
    "            feed_dict)\n",
    "        \n",
    "        #if self.summary_writer and episode_num % report_frequency == 0:\n",
    "        #    self.summary_writer.add_summary(summaries, global_step)\n",
    "        return loss, self.optimizer._lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetworkCopier():\n",
    "    \"\"\"\n",
    "    Copy model parameters of one estimator to another.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, estimator1, estimator2):\n",
    "        \"\"\"\n",
    "        Defines copy-work operation graph.  \n",
    "        Args:\n",
    "          estimator1: Estimator to copy the paramters from\n",
    "          estimator2: Estimator to copy the parameters to\n",
    "        \"\"\"\n",
    "        e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)]\n",
    "        e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "        e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)]\n",
    "        e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "\n",
    "        self.update_ops = []\n",
    "        tau = 1e-3\n",
    "        self.soft_update_ops = []\n",
    "        for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "            op = e2_v.assign(e1_v)\n",
    "            self.update_ops.append(op)\n",
    "            op_soft = e2_v.assign(tau*e1_v + (1.0-tau)*e2_v)\n",
    "            self.soft_update_ops.append(op_soft)\n",
    "            \n",
    "    def update(self, sess):\n",
    "        \"\"\"\n",
    "        Makes copy.\n",
    "        Args:\n",
    "            sess: Tensorflow session instance\n",
    "        \"\"\"\n",
    "        sess.run(self.update_ops)\n",
    "    \n",
    "    def soft_update(self, sess):\n",
    "        \"\"\"\n",
    "        Makes soft copy.\n",
    "        Args:\n",
    "            sess: Tensorflow session instance\n",
    "        \"\"\"\n",
    "        sess.run(self.soft_update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "\n",
    "    Args:\n",
    "        estimator: An estimator that returns q values for a given state\n",
    "        nA: Number of actions in the environment.\n",
    "\n",
    "    Returns:\n",
    "        A function that takes the (sess, observation, epsilon) as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "\n",
    "    \"\"\"\n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        # take uniformly selected random action\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.choice(np.arange(nA))\n",
    "\n",
    "        # Pick the action with highest q value.\n",
    "        q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
    "        \n",
    "        #best_action = np.argmax(q_values)\n",
    "        \n",
    "        max_q = max(q_values)\n",
    "        # In case multiple actions have the same maximum q value.\n",
    "        actions_with_max_q = [] \n",
    "        for a in range(0, nA):\n",
    "            if q_values[a] == max_q:\n",
    "                actions_with_max_q.append(a)\n",
    "        #print(actions_with_max_q)\n",
    "        return np.random.choice(actions_with_max_q)\n",
    "        \n",
    "        #A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        #q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
    "        #best_action = np.argmax(q_values)\n",
    "        \n",
    "        #A[best_action] += (1.0 - epsilon)\n",
    "        \n",
    "        #A = A / np.sum(A)\n",
    "        \n",
    "        #if np.abs(1.0 - np.sum(A)) > 1e-3:\n",
    "        #    print(A[best_action], np.sum(A))\n",
    "        \n",
    "        #return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "class deep_q_learning():\n",
    "    def __init__(self, sess,\n",
    "                 q_estimator,\n",
    "                 target_estimator,\n",
    "                 num_episodes,\n",
    "                 experiment_dir,\n",
    "                 replay_memory_size=500000,\n",
    "                 discount_factor=0.99,\n",
    "                 epsilon_start=1.0,\n",
    "                 epsilon_end=0.1,\n",
    "                 epsilon_decay_steps=500000,\n",
    "                 batch_size=32):\n",
    "        \"\"\"\n",
    "        Q-Learning algorithm for off-policy TD control using Function Approximation.\n",
    "        Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "\n",
    "        Args:\n",
    "            sess: Tensorflow Session object\n",
    "            q_estimator: Estimator object used for the q values\n",
    "            target_estimator: Estimator object used for the targets\n",
    "            num_episodes: Number of episodes to run for\n",
    "            experiment_dir: Directory to save Tensorflow summaries in\n",
    "            replay_memory_size: Size of the replay memory\n",
    "            discount_factor: Gamma discount factor\n",
    "            epsilon_start: Chance to sample a random action when taking an action.\n",
    "                           Epsilon is decayed over time and this is the start value\n",
    "            epsilon_end: The final minimum value of epsilon after decaying is done\n",
    "            epsilon_decay_steps: Number of steps to decay epsilon over\n",
    "            batch_size: Size of batches to sample from the replay memory\n",
    "        \"\"\"\n",
    "        \n",
    "        self.sess = sess\n",
    "        self.q_estimator = q_estimator\n",
    "        self.target_estimator = target_estimator\n",
    "        self.num_episodes = num_episodes\n",
    "        self.experiment_dir = experiment_dir\n",
    "        self.replay_memory_size = replay_memory_size\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay_steps = epsilon_decay_steps\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.episode_reward = 0\n",
    "        self.cur_t = 0\n",
    "        self.use_double_dqn = True\n",
    "        \n",
    "        # The replay memory\n",
    "        self.replay_memory = deque(maxlen=int(replay_memory_size))\n",
    "        \n",
    "        # Make model copier object\n",
    "        self.estimator_copy = QNetworkCopier(self.q_estimator, self.target_estimator)\n",
    "\n",
    "        # Keeps track of useful statistics\n",
    "        self.stats = {'q_net_loss':deque(maxlen=500), 'q_net_lr':deque(maxlen=500),\n",
    "                      'episode_rewards':deque(maxlen=500), 'epsilon':deque(maxlen=500)}\n",
    "\n",
    "        # Create directories for checkpoints and summaries\n",
    "        self.checkpoint_dir = os.path.join(self.experiment_dir, \"checkpoints\")\n",
    "        self.checkpoint_path = os.path.join(self.checkpoint_dir, \"model\")\n",
    "\n",
    "        if not os.path.exists(self.checkpoint_dir):\n",
    "            os.makedirs(self.checkpoint_dir)\n",
    "\n",
    "        self.saver = tf.train.Saver()\n",
    "        # Load a previous checkpoint if we find one\n",
    "        latest_checkpoint = tf.train.latest_checkpoint(self.checkpoint_dir)\n",
    "        if latest_checkpoint:\n",
    "            print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "            self.saver.restore(self.sess, latest_checkpoint)\n",
    "\n",
    "        # Get the current time step\n",
    "        self.total_t = sess.run(tf.contrib.framework.get_global_step())\n",
    "        \n",
    "        self.cur_episode = int(self.total_t / 25)\n",
    "        \n",
    "        # The epsilon decay schedule\n",
    "        self.epsilons = np.linspace(self.epsilon_start, self.epsilon_end, self.epsilon_decay_steps)\n",
    "\n",
    "        # The policy we're following\n",
    "        self.policy = make_epsilon_greedy_policy(self.q_estimator, N_All_Actions)\n",
    "    \n",
    "    def collect_observation(self, board, move, score_delta, next_board, moves_left):\n",
    "        state = get_state(board, moves_left + 1)\n",
    "        reward = np.sum(np.sum(np.abs(get_board_num(board) - get_board_num(next_board)))) / 80.0\n",
    "        action = get_action_from(move)\n",
    "        n_state = get_state(next_board, moves_left)\n",
    "        done = (moves_left == 0)\n",
    "        \n",
    "        # Update statistics\n",
    "        self.episode_reward += score_delta\n",
    "        \n",
    "        ## If our replay memory is full, pop the first element\n",
    "        self.replay_memory.append(Transition(state, action, reward, n_state, done))\n",
    "        \n",
    "        self.cur_t += 1\n",
    "        self.total_t += 1\n",
    "        return\n",
    "    \n",
    "    def predict_action(self, board, score, moves_left):\n",
    "        state = get_state(board, moves_left)\n",
    "        epsilon = self.epsilons[min(self.total_t, self.epsilon_decay_steps-1)]\n",
    "        action = self.policy(self.sess, state, epsilon)\n",
    "        \n",
    "        self.stats['epsilon'].append(epsilon)\n",
    "        return get_move_from(All_Actions[action])\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.replay_memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        if self.cur_t % 16 != 0:\n",
    "            return\n",
    "        \n",
    "        # Sample a minibatch from the replay memory\n",
    "        samples = random.sample(self.replay_memory, int(self.batch_size))\n",
    "        states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n",
    "        \n",
    "        # Calculate q values and targets\n",
    "        if not self.use_double_dqn:\n",
    "            q_values_next = self.target_estimator.predict(self.sess, next_states_batch)\n",
    "            targets_batch = reward_batch \\\n",
    "                          + np.invert(done_batch).astype(np.float32) \\\n",
    "                          * self.discount_factor \\\n",
    "                          * np.amax(q_values_next, axis=1)\n",
    "        else:\n",
    "            q_values_next = self.q_estimator.predict(self.sess, next_states_batch)\n",
    "            best_actions = np.argmax(q_values_next, axis=1)\n",
    "            q_values_next_target = self.target_estimator.predict(self.sess, next_states_batch)\n",
    "            targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * \\\n",
    "                self.discount_factor * q_values_next_target[np.arange(self.batch_size), best_actions]\n",
    "        \n",
    "        #print(reward_batch, q_values_next)\n",
    "        \n",
    "        # Perform gradient descent update\n",
    "        states_batch = np.array(states_batch)\n",
    "        loss, lr = self.q_estimator.update(self.sess, states_batch, action_batch, targets_batch)\n",
    "        self.estimator_copy.soft_update(self.sess)\n",
    "        \n",
    "        self.stats['q_net_loss'].append(loss)\n",
    "        self.stats['q_net_lr'].append(lr)\n",
    "        \n",
    "        # Print out which step we're on, useful for debugging.\n",
    "        print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n",
    "                self.cur_t, self.total_t, self.cur_episode + 1, self.num_episodes, np.mean(self.stats['q_net_loss'])), end=\"\")\n",
    "    \n",
    "    def init_episode(self):\n",
    "        self.stats['episode_rewards'].append(self.episode_reward)\n",
    "        \n",
    "        # Reset\n",
    "        self.cur_t = 0\n",
    "        self.cur_episode += 1\n",
    "        self.episode_reward = 0\n",
    "        \n",
    "        if len(self.replay_memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        if self.cur_episode % 10 == 0:\n",
    "            # Save the current checkpoint\n",
    "            self.saver.save(self.sess, self.checkpoint_path)\n",
    "        \n",
    "        # Add summaries to tensorboard\n",
    "        episode_summary = tf.Summary()\n",
    "        episode_summary.value.add(\n",
    "            simple_value = np.mean(self.stats['epsilon']), tag=\"episode/epsilon\")\n",
    "        episode_summary.value.add(\n",
    "            simple_value = np.mean(self.stats['episode_rewards']), tag=\"episode/reward\")\n",
    "        episode_summary.value.add(\n",
    "            simple_value = np.mean(self.stats['q_net_loss']), tag=\"QNet/Loss\")\n",
    "        episode_summary.value.add(\n",
    "            simple_value = np.mean(self.stats['q_net_lr']), tag=\"QNet/LR\")\n",
    "        q_estimator.summary_writer.add_summary(episode_summary, self.cur_episode)\n",
    "        q_estimator.summary_writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-fd064249b028>:54: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_global_step\n",
      "Loading model checkpoint C:\\Kourosh\\Project\\Ubisoft\\RedLynx_ML_Assignment\\experiments\\ubisoft-game\\checkpoints\\model...\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from C:\\Kourosh\\Project\\Ubisoft\\RedLynx_ML_Assignment\\experiments\\ubisoft-game\\checkpoints\\model\n",
      "Seed: 275554838359217849107027030859888925544\n",
      "Step 16 (806359) @ Episode 32254/50000, loss: 0.88576990365982064"
     ]
    }
   ],
   "source": [
    "global my_dqn\n",
    "\n",
    "def ai_callback(board, score, moves_left):\n",
    "    global my_dqn\n",
    "    \n",
    "    predicted_move = my_dqn.predict_action(board, score, moves_left)\n",
    "    #print(predicted_move)\n",
    "    #dir = random.randint(0, 1) == 0\n",
    "    #return (random.randint(0, 7 if dir else 6), random.randint(0, 8 if dir else 9), dir)\n",
    "    return predicted_move\n",
    "\n",
    "def transition_callback(board, move, score_delta, next_board, moves_left):\n",
    "    global my_dqn\n",
    "    \n",
    "    my_dqn.collect_observation(board, move, score_delta, next_board, moves_left)\n",
    "    my_dqn.update()\n",
    "    \n",
    "    pass # This can be used to monitor outcomes of moves\n",
    "\n",
    "def end_of_game_callback(boards, scores, moves, final_score):\n",
    "    global my_dqn\n",
    "    \n",
    "    my_dqn.init_episode()\n",
    "    \n",
    "    return True # True = play another, False = Done\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    global my_dqn\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Where we save our checkpoints and graphs\n",
    "    experiment_dir = os.path.abspath(\"./experiments/{}\".format(\"ubisoft-game\"))\n",
    "\n",
    "    # Create a glboal step variable\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "    # Create estimators\n",
    "    q_estimator = QNetwork(init_learning_rate=1e-3, scope=\"q_estimator\", summaries_dir=experiment_dir)\n",
    "    \n",
    "    target_estimator = QNetwork(scope=\"target_q\")\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    my_dqn = deep_q_learning(sess,\n",
    "                             q_estimator=q_estimator,\n",
    "                             target_estimator=target_estimator,\n",
    "                             experiment_dir=experiment_dir,\n",
    "                             num_episodes=50000,\n",
    "                             replay_memory_size=1000000,\n",
    "                             epsilon_start=1.0,\n",
    "                             epsilon_end=0.1,\n",
    "                             epsilon_decay_steps=500000,\n",
    "                             discount_factor=0.99,\n",
    "                             batch_size=64)\n",
    "\n",
    "    speedup = 1000.0\n",
    "    g = graphical.Game(ai_callback, transition_callback, end_of_game_callback, speedup)\n",
    "    g.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
