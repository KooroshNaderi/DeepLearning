{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import graphical, game\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed actions: [70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 89, 99, 109, 119, 129, 139, 149, 159]\n",
      "number of errors happens in action conversion:  0\n",
      "removed for example: (7, 9, True) (7, 3, False)\n",
      "Num of state:  160 , Num of actions:  142\n"
     ]
    }
   ],
   "source": [
    "N_Rows = 10\n",
    "N_Cols = 8\n",
    "N_Dir = 2\n",
    "\n",
    "def match_in_row_from_index(board_num, i, j, flag_print):\n",
    "    count_equal_forward = 1\n",
    "    count_equal_backward = 0\n",
    "    c1 = board_num[i, j]\n",
    "    if c1 < 0:\n",
    "        return False\n",
    "    \n",
    "    if j + 1 < N_Cols and c1 == board_num[i, j + 1]:\n",
    "        count_equal_forward += 1\n",
    "        if j + 2 < N_Cols and c1 == board_num[i, j + 2]:\n",
    "            count_equal_forward += 1\n",
    "    \n",
    "    if j - 1 >= 0 and c1 == board_num[i, j - 1]:\n",
    "        count_equal_backward += 1\n",
    "        if j - 2 >= 0 and c1 == board_num[i, j - 2]:\n",
    "            count_equal_backward += 1\n",
    "    if flag_print:\n",
    "        print(count_equal_backward + count_equal_forward)\n",
    "    \n",
    "    if count_equal_backward + count_equal_forward >= 3:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def match_in_col_from_index(board_num, i, j, flag_print):\n",
    "    count_equal_forward = 1\n",
    "    count_equal_backward = 0\n",
    "    r1 = board_num[i, j]\n",
    "    if r1 < 0:\n",
    "        return False\n",
    "    \n",
    "    if i + 1 < N_Rows and r1 == board_num[i + 1, j]:\n",
    "        count_equal_forward += 1\n",
    "        if i + 2 < N_Rows and r1 == board_num[i + 2, j]:\n",
    "            count_equal_forward += 1\n",
    "    \n",
    "    if i - 1 >= 0 and r1 == board_num[i - 1, j]:\n",
    "        count_equal_backward += 1\n",
    "        if i - 2 >= 0 and r1 == board_num[i - 2, j]:\n",
    "            count_equal_backward += 1\n",
    "    \n",
    "    if flag_print:\n",
    "        print(count_equal_backward + count_equal_forward)\n",
    "    \n",
    "    if count_equal_backward + count_equal_forward >= 3:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def swap_col(board_num, row, col):\n",
    "    tmp = board_num[row, col + 1]\n",
    "    board_num[row, col + 1] = board_num[row, col]\n",
    "    board_num[row, col] = tmp\n",
    "    return board_num\n",
    "\n",
    "def swap_row(board_num, row, col):\n",
    "    tmp = board_num[row + 1, col]\n",
    "    board_num[row + 1, col] = board_num[row, col]\n",
    "    board_num[row, col] = tmp\n",
    "    return board_num\n",
    "\n",
    "def get_board_num(board):\n",
    "    #print(board)\n",
    "    board_num = np.zeros((N_Rows, N_Cols))\n",
    "    col = 0\n",
    "    row = 0\n",
    "    for s in range(0, len(board)):\n",
    "        if board[s] != '\\n':\n",
    "            board_num[row, col] = ord(board[s]) - ord('a')\n",
    "            board_num[row, col] = -1 if board_num[row, col] < 0 else board_num[row, col]\n",
    "            \n",
    "            col += 1\n",
    "        else:\n",
    "            col = 0\n",
    "            row += 1\n",
    "    #print(board_num)\n",
    "    return board_num\n",
    "\n",
    "def get_game_state(board, moves_left):\n",
    "    board_num = get_board_num(board)\n",
    "    \n",
    "    flag_print = False\n",
    "    col_feature = np.zeros((N_Rows, N_Cols))\n",
    "    for row in range(0, N_Rows):\n",
    "        for col in range(0, N_Cols - 1):\n",
    "            #flag_print = False\n",
    "            #if row == 3 and col == 3:\n",
    "            #    flag_print = True\n",
    "            \n",
    "            board_num = swap_col(board_num, row, col)\n",
    "            if flag_print:\n",
    "                print(board_num)\n",
    "            if (match_in_row_from_index(board_num, row, col, flag_print) or\n",
    "                match_in_col_from_index(board_num, row, col, flag_print) or\n",
    "                match_in_row_from_index(board_num, row, col + 1, flag_print) or\n",
    "                match_in_col_from_index(board_num, row, col + 1, flag_print)):\n",
    "                col_feature[row, col] = 1\n",
    "            board_num = swap_col(board_num, row, col)\n",
    "            if flag_print:\n",
    "                print(board_num)\n",
    "    #print(col_feature)\n",
    "    \n",
    "    row_feature = np.zeros((N_Rows, N_Cols))\n",
    "    for row in range(0, N_Rows - 1):\n",
    "        for col in range(0, N_Cols):\n",
    "            #flag_print = False\n",
    "            #if row == 3 and col == 3:\n",
    "            #    flag_print = True\n",
    "            \n",
    "            board_num = swap_row(board_num, row, col)\n",
    "            if flag_print:\n",
    "                print(board_num)\n",
    "            if (match_in_row_from_index(board_num, row, col, flag_print) or\n",
    "                match_in_col_from_index(board_num, row, col, flag_print) or\n",
    "                match_in_row_from_index(board_num, row + 1, col, flag_print) or\n",
    "                match_in_col_from_index(board_num, row + 1, col, flag_print)):\n",
    "                row_feature[row, col] = 1\n",
    "            board_num = swap_row(board_num, row, col)\n",
    "            if flag_print:\n",
    "                print(board_num)\n",
    "    #print(row_feature)\n",
    "    \n",
    "    ###### put features in the state\n",
    "    state = np.zeros(2 * (N_Rows * N_Cols))\n",
    "    \n",
    "    c_state_index = 0\n",
    "    for row in range(0, N_Rows):\n",
    "        for col in range(0, N_Cols):\n",
    "            v = 0\n",
    "            if col_feature[row, col] and row_feature[row, col]:\n",
    "                v = 1\n",
    "            elif col_feature[row, col]:\n",
    "                v = 2\n",
    "            else:\n",
    "                v = 3\n",
    "            state[c_state_index] = 2.0 * (v / 3.0) - 1.0\n",
    "            c_state_index += 1\n",
    "            state[c_state_index] = 2.0 * ((board_num[row, col] + 1) / 5.0) - 1.0\n",
    "            c_state_index += 1\n",
    "\n",
    "    #state[c_state_index] = moves_left / 25.0\n",
    "    \n",
    "    return state\n",
    "\n",
    "def get_action_from(move):\n",
    "    action = np.array(move)\n",
    "    \n",
    "    if move[2]:\n",
    "        action[2] = 1\n",
    "    else:\n",
    "        action[2] = 0\n",
    "    \n",
    "    out_action = (action[2]) * (N_Rows * N_Cols) + (action[0] * N_Rows + action[1])\n",
    "    \n",
    "    return out_action\n",
    "\n",
    "def get_move_from(action):\n",
    "    row_col = action % (N_Rows * N_Cols)\n",
    "    \n",
    "    dir = int(action / (N_Rows * N_Cols))\n",
    "    \n",
    "    return (int(row_col / N_Rows), row_col % N_Rows, dir >= 1)\n",
    "\n",
    "def get_valid_actions():\n",
    "    All_Actions = np.arange(0,N_Rows*N_Cols*N_Dir)\n",
    "    #print(All_Actions)\n",
    "    \n",
    "    remove_ids = []\n",
    "    for i in range(0, N_Rows):\n",
    "        action_id = get_action_from((N_Cols - 1, i, False))\n",
    "        remove_ids.append(action_id)\n",
    "    \n",
    "    for i in range(0, N_Cols):\n",
    "        action_id = get_action_from((i, N_Rows - 1, True))\n",
    "        remove_ids.append(action_id)\n",
    "    print(\"removed actions:\", remove_ids)\n",
    "    return np.delete(All_Actions, remove_ids, axis=0), remove_ids\n",
    "\n",
    "All_Actions, remove_ids = get_valid_actions()\n",
    "#print(All_Actions)\n",
    "\n",
    "board_sample_str = \"da#bb#ac\\n#bbccbbd\\n#cd#a#d#\\n#c#d#ddc\\n##ba##bc\\nacadc#c#\\n#d##cc##\\nc#cbdacd\\ndca#d#b#\\ndd#dccdb\"\n",
    "state = get_state(board_sample_str, 0)\n",
    "\n",
    "N_All_Actions = len(All_Actions)\n",
    "N_State = len(state)\n",
    "\n",
    "###################################################### test ############################################\n",
    "\n",
    "# test action conversion\n",
    "num_error_in_conversion = 0\n",
    "for i in range(0,160):\n",
    "    a = get_move_from(i)\n",
    "    ii = get_action_from(a)\n",
    "    if i != ii:\n",
    "        num_error_in_conversion += 1\n",
    "print(\"number of errors happens in action conversion: \", num_error_in_conversion)\n",
    "\n",
    "print(\"removed for example:\", get_move_from(159), get_move_from(73))\n",
    "\n",
    "print(\"Num of state: \", N_State, \", Num of actions: \", N_All_Actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, \n",
    "                 #summaries_dir, \n",
    "                 #policy_type, \n",
    "                 action_size, \n",
    "                 state_size, \n",
    "                 max_steps, \n",
    "                 #use_autoencoder_as_feature_selection,\n",
    "                 global_step):\n",
    "        #if summaries_dir:\n",
    "        #    if not os.path.exists(summaries_dir):\n",
    "        #        os.makedirs(summaries_dir)\n",
    "        #    self.summary_writer = tf.summary.FileWriter(summaries_dir)\n",
    "        \n",
    "        self.global_step = global_step\n",
    "        self.max_steps = max_steps\n",
    "        self.action_size = action_size\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        self.state = tf.placeholder(shape=[None, N_State], dtype=tf.float32, name=\"state\")\n",
    "        #self.next_state = tf.placeholder(shape=[None, N_State], dtype=tf.float32, name=\"next_state\")\n",
    "        self.action = tf.placeholder(shape=[None], dtype=tf.float32, name=\"action\")\n",
    "        \n",
    "        #self.activation = tf.nn.relu\n",
    "        #self.policy_type = policy_type\n",
    "        \n",
    "        # Policy and value network        \n",
    "        #self.use_autoencoder_as_feature_selection = use_autoencoder_as_feature_selection\n",
    "        self.model_summaries = {'learning_rate':deque(maxlen=500),\n",
    "                                'beta':deque(maxlen=500),\n",
    "                                'policy_loss':deque(maxlen=500),\n",
    "                                'entropy':deque(maxlen=500),\n",
    "                                'value_loss':deque(maxlen=500)}\n",
    "        #if self.use_autoencoder_as_feature_selection:\n",
    "        #    encoded_state = self.u_net(input_image=self.state, reuse=False, scope='')\n",
    "        #    #encoded_next_state = self.u_net(input_image=self.next_state, reuse=True, scope='')\n",
    "        #    self.PolicyEstimator(tf.stop_gradient(encoded_state))\n",
    "        #    self.ValueEstimator(tf.stop_gradient(encoded_state))\n",
    "        #else:\n",
    "        encoded_state = self.state_processor(input_X=self.state)\n",
    "        self.PolicyEstimator(encoded_state)\n",
    "        self.ValueEstimator(encoded_state)\n",
    "\n",
    "        self.define_loss()\n",
    "        self.set_graph=False\n",
    "    \n",
    "    def state_processor(self, input_X):\n",
    "        hidden_layer_size = 64\n",
    "        with tf.variable_scope(\"encoded_state\"):\n",
    "            fully1 = tf.layers.dense(input_X, hidden_layer_size, activation=tf.nn.relu)      \n",
    "            fully2 = tf.layers.dense(fully1, hidden_layer_size, activation=tf.nn.relu) \n",
    "            fully3 = tf.layers.dense(fully2, hidden_layer_size, activation=tf.nn.relu)\n",
    "        return fully3\n",
    "    \n",
    "    def PolicyEstimator(self, encoded_state):\n",
    "        use_continuous = True\n",
    "        with tf.variable_scope(\"policy_estimator\"):\n",
    "            \n",
    "            self.advantage = tf.placeholder(\n",
    "                shape=[None],\n",
    "                dtype=tf.float32,\n",
    "                name=\"advantage\"\n",
    "            )\n",
    "            \n",
    "            if use_continuous:\n",
    "                mu = tf.layers.dense(\n",
    "                    encoded_state,\n",
    "                    self.action_size,\n",
    "                    activation=None,\n",
    "                    kernel_initializer=LearningModel.scaled_init(0.01),\n",
    "                    reuse=tf.AUTO_REUSE,\n",
    "                )\n",
    "\n",
    "                self.log_sigma_sq = tf.get_variable(\n",
    "                    \"log_sigma_squared\",\n",
    "                    [self.action_size],\n",
    "                    dtype=tf.float32,\n",
    "                    initializer=tf.zeros_initializer(),\n",
    "                )\n",
    "\n",
    "                sigma_sq = tf.exp(self.log_sigma_sq)\n",
    "\n",
    "                self.epsilon = tf.placeholder(\n",
    "                    shape=[None, self.action_size], dtype=tf.float32, name=\"epsilon\"\n",
    "                )\n",
    "                # Clip and scale output to ensure actions are always within [-1, 1] range.\n",
    "                self.output_pre = mu + tf.sqrt(sigma_sq) * self.epsilon\n",
    "                output_post = tf.clip_by_value(self.output_pre, -3, 3) / 3\n",
    "                self.output = tf.identity(output_post, name=\"action\")\n",
    "                self.selected_actions = tf.stop_gradient(output_post)\n",
    "                # Compute probability of model output.\n",
    "                all_probs = (\n",
    "                    -0.5 * tf.square(tf.stop_gradient(self.output_pre) - mu) / sigma_sq\n",
    "                    - 0.5 * tf.log(2.0 * np.pi)\n",
    "                    - 0.5 * self.log_sigma_sq\n",
    "                )\n",
    "\n",
    "                self.all_log_probs = tf.identity(all_probs, name=\"action_probs\")\n",
    "\n",
    "                self.entropy = 0.5 * tf.reduce_mean(\n",
    "                    tf.log(2 * np.pi * np.e) + self.log_sigma_sq\n",
    "                )\n",
    "\n",
    "                self.all_old_log_probs = tf.placeholder(\n",
    "                    shape=[None, self.act_size[0]], dtype=tf.float32, name=\"old_probabilities\"\n",
    "                )\n",
    "\n",
    "                # We keep these tensors the same name, but use new nodes to keep code parallelism with discrete control.\n",
    "                self.log_probs = tf.reduce_sum(\n",
    "                    (tf.identity(self.all_log_probs)), axis=1, keepdims=True\n",
    "                )\n",
    "                self.old_log_probs = tf.reduce_sum(\n",
    "                    (tf.identity(self.all_old_log_probs)), axis=1, keepdims=True\n",
    "                )\n",
    "            else:\n",
    "                self.old_action_probs = tf.placeholder(shape=[None, self.action_size], \n",
    "                                                       dtype=tf.float32, \n",
    "                                                       name=\"old_action_probabilities\")\n",
    "\n",
    "                self.action_probs = tf.layers.dense(encoded_state, \n",
    "                                                    self.action_size,\n",
    "                                                    activation=tf.nn.softmax,\n",
    "                                                    kernel_initializer=tf.contrib.layers.variance_scaling_initializer(0.01))\n",
    "\n",
    "                self.picked_action_prob = tf.reduce_sum(self.action_probs *\\\n",
    "                                                        tf.one_hot(self.action, self.action_size),\n",
    "                                                        axis=1)\n",
    "                self.picked_old_action_prob = tf.reduce_sum(self.old_action_probs *\\\n",
    "                                                            tf.one_hot(self.action, self.action_size),\n",
    "                                                            axis=1)\n",
    "                self.entropy = -tf.reduce_sum(self.action_probs *\\\n",
    "                                              tf.log(self.action_probs + 1e-10),\n",
    "                                              axis=1)\n",
    "                self.mean_entropy = tf.reduce_mean(self.entropy)\n",
    "\n",
    "                self.decay_epsilon = tf.train.polynomial_decay(0.2, self.global_step, self.max_steps, 0.1, power=1.0)\n",
    "                #Clipped Surrogate Objective\n",
    "                ratio = self.picked_action_prob / (self.picked_old_action_prob  + 1e-10)\n",
    "                a = ratio * self.advantage\n",
    "                b = tf.clip_by_value(ratio, 1.0 - self.decay_epsilon, 1.0 + self.decay_epsilon) * self.advantage\n",
    "                self.policy_loss = -tf.reduce_mean(tf.minimum(a, b))\n",
    "                self.abs_policy_loss = abs(self.policy_loss)\n",
    "            \n",
    "    def policy_predict(self, sess, state):\n",
    "        if not self.set_graph:\n",
    "            self.summary_writer.add_graph(sess.graph)\n",
    "            self.set_graph = True\n",
    "        return sess.run(self.action_probs, {self.state: state})\n",
    "    \n",
    "    def ValueEstimator(self, encoded_state):\n",
    "        with tf.variable_scope(\"value_estimator\"):\n",
    "            self.target = tf.placeholder(shape=[None], dtype=tf.float32, name=\"target\")\n",
    "            self.old_value = tf.placeholder(shape=[None], dtype=tf.float32, name=\"old_values\")\n",
    "            self.value_estimate = tf.layers.dense(encoded_state, 1, activation=None)\n",
    "            self.value_estimate = tf.squeeze(self.value_estimate, axis=1)\n",
    "            \n",
    "            a = tf.squared_difference(self.value_estimate, self.target)\n",
    "            clipped_value_estimate = self.old_value + tf.clip_by_value(self.value_estimate - self.old_value , \n",
    "                                                                       -self.decay_epsilon, self.decay_epsilon)\n",
    "            b = tf.squared_difference(clipped_value_estimate, self.target)\n",
    "            self.value_loss = tf.reduce_mean(tf.minimum(a, b))\n",
    "            \n",
    "    def value_predict(self, sess, state):\n",
    "        return sess.run(self.value_estimate, {self.state: state})\n",
    "    \n",
    "    def define_loss(self, learning_rate=1e-4):\n",
    "        self.decay_learning_rate = tf.train.polynomial_decay(learning_rate, self.global_step, self.max_steps, 1e-10, power=1.0)\n",
    "        self.decay_beta = tf.train.polynomial_decay(5e-3, self.global_step, self.max_steps, 1e-5, power=1.0)\n",
    "        \n",
    "        self.loss = self.policy_loss + 0.5 * self.value_loss - self.decay_beta * tf.reduce_mean(self.entropy)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.decay_learning_rate)\n",
    "        self.train_op = optimizer.minimize(self.loss)\n",
    "            \n",
    "    def update_model(self, sess, state, next_state, action, old_action_probs, target, advantage, old_value):\n",
    "        feed_dict = {self.state: state, self.next_state: next_state, \n",
    "                     self.action: action, self.old_action_probs: old_action_probs, \n",
    "                     self.target: target, self.advantage: advantage, self.old_value: old_value}\n",
    "        \n",
    "        lr, beta, abs_policy_loss, entropy, value_loss, _, _, loss = \\\n",
    "            sess.run([self.decay_learning_rate, self.decay_beta, \n",
    "                      self.abs_policy_loss, self.mean_entropy,\n",
    "                      self.value_loss, tf.train.get_global_step(), \n",
    "                      self.train_op, self.loss], feed_dict)\n",
    "        self.model_summaries['learning_rate'].append(lr)\n",
    "        self.model_summaries['beta'].append(beta)\n",
    "        self.model_summaries['policy_loss'].append(abs_policy_loss)\n",
    "        self.model_summaries['entropy'].append(entropy)\n",
    "        self.model_summaries['value_loss'].append(value_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, \n",
    "                 max_steps=10e+6, \n",
    "                 total_episodes=100000, \n",
    "                 discount_factor=0.99):\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        global_step_tensor = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        sess = tf.Session()\n",
    "        sess.run(global_step_tensor.initializer)\n",
    "    \n",
    "        state_size = N_State\n",
    "        action_size = 3\n",
    "        self.sess = sess\n",
    "        #self.model = Model(summaries_dir='./experiments_' + policy_type + '_' + env_name + '/summaries', \n",
    "        #                   policy_type=policy_type, action_size=action_size, state_size=state_size,\n",
    "        #                   max_steps=max_steps, use_autoencoder_as_feature_selection=use_autoencoder_as_feature_selection, \n",
    "        #                   global_step=global_step_tensor)\n",
    "        \n",
    "        self.model = Model(action_size=action_size,\n",
    "                           state_size=state_size,\n",
    "                           max_steps=max_steps, \n",
    "                           global_step=global_step_tensor)\n",
    "        \n",
    "        #first build the model, then initialize variables\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.total_episodes = total_episodes\n",
    "        self.max_replay_memory_size = 10240\n",
    "        self.batch_size = 1024\n",
    "        self.epochs = 3\n",
    "        self.discount_factor = discount_factor\n",
    "        self.lambda_ = 0.95\n",
    "        \n",
    "        self.checkpoint_dir = os.path.join('./experiments', 'checkpoints')\n",
    "        self.checkpoint_path = os.path.join(self.checkpoint_dir, 'model')\n",
    "        if not os.path.exists(self.checkpoint_dir):\n",
    "            os.makedirs(self.checkpoint_dir)\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.load_model(self.checkpoint_dir)        \n",
    "    \n",
    "    def save_model():\n",
    "        pass\n",
    "    \n",
    "    def load_model(self):\n",
    "        self.latest_checkpoint = tf.train.latest_checkpoint(self.checkpoint_dir)\n",
    "        if self.latest_checkpoint:\n",
    "            print(\"Loading checkpoint\")\n",
    "            self.saver.restore(self.sess, self.latest_checkpoint)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        action_prob = self.model.policy_predict(self.sess, np.expand_dims(state,axis=0))[0]\n",
    "        action = np.random.choice(np.arange(len(action_prob)), p=action_prob)\n",
    "        \n",
    "        return action, action_prob\n",
    "    \n",
    "    def predict_action(self, board, score, moves_left):\n",
    "        \n",
    "        state = get_game_state(board, moves_left)\n",
    "        \n",
    "        action, action_prob = self.get_action(state)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def collect_observation(self,board, move, score_delta, next_board, moves_left):\n",
    "        pass\n",
    "    \n",
    "    def update(self):\n",
    "        pass\n",
    "    \n",
    "    def init_episode(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-fd064249b028>:54: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_global_step\n",
      "Loading model checkpoint C:\\Kourosh\\Project\\Ubisoft\\RedLynx_ML_Assignment\\experiments\\ubisoft-game\\checkpoints\\model...\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from C:\\Kourosh\\Project\\Ubisoft\\RedLynx_ML_Assignment\\experiments\\ubisoft-game\\checkpoints\\model\n",
      "Seed: 275554838359217849107027030859888925544\n",
      "Step 16 (806359) @ Episode 32254/50000, loss: 0.88576990365982064"
     ]
    }
   ],
   "source": [
    "global my_dqn\n",
    "\n",
    "def ai_callback(board, score, moves_left):\n",
    "    global my_dqn\n",
    "    \n",
    "    predicted_move = my_dqn.predict_action(board, score, moves_left)\n",
    "    #print(predicted_move)\n",
    "    #dir = random.randint(0, 1) == 0\n",
    "    #return (random.randint(0, 7 if dir else 6), random.randint(0, 8 if dir else 9), dir)\n",
    "    return predicted_move\n",
    "\n",
    "def transition_callback(board, move, score_delta, next_board, moves_left):\n",
    "    global my_dqn\n",
    "    \n",
    "    my_dqn.collect_observation(board, move, score_delta, next_board, moves_left)\n",
    "    my_dqn.update()\n",
    "    \n",
    "    pass # This can be used to monitor outcomes of moves\n",
    "\n",
    "def end_of_game_callback(boards, scores, moves, final_score):\n",
    "    global my_dqn\n",
    "    \n",
    "    my_dqn.init_episode()\n",
    "    \n",
    "    return True # True = play another, False = Done\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    global my_dqn\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Where we save our checkpoints and graphs\n",
    "    experiment_dir = os.path.abspath(\"./experiments/{}\".format(\"ubisoft-game\"))\n",
    "\n",
    "    # Create a glboal step variable\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "    # Create estimators\n",
    "    q_estimator = QNetwork(init_learning_rate=1e-3, scope=\"q_estimator\", summaries_dir=experiment_dir)\n",
    "    \n",
    "    target_estimator = QNetwork(scope=\"target_q\")\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    my_dqn = deep_q_learning(sess,\n",
    "                             q_estimator=q_estimator,\n",
    "                             target_estimator=target_estimator,\n",
    "                             experiment_dir=experiment_dir,\n",
    "                             num_episodes=50000,\n",
    "                             replay_memory_size=1000000,\n",
    "                             epsilon_start=1.0,\n",
    "                             epsilon_end=0.1,\n",
    "                             epsilon_decay_steps=500000,\n",
    "                             discount_factor=0.99,\n",
    "                             batch_size=64)\n",
    "\n",
    "    speedup = 1000.0\n",
    "    g = graphical.Game(ai_callback, transition_callback, end_of_game_callback, speedup)\n",
    "    g.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
